{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iixApa18PT4B"
      },
      "source": [
        "# Set LLM API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWyI0-GxNezi"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFSJ144TvVgo"
      },
      "outputs": [],
      "source": [
        "!pip install pipreqs\n",
        "!pip install pipreqsnb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJnfR-V_N5pY"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import pandas as pd\n",
        "import openai\n",
        "import os\n",
        "from difflib import SequenceMatcher\n",
        "import time\n",
        "import re\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import ranksums\n",
        "import json\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCO6VnAplYAu"
      },
      "outputs": [],
      "source": [
        "# put the huggingface api key\n",
        "openai_key=\"\"\n",
        "os.environ['OPENAI_API_KEY'] = openai_key\n",
        "openai.api_key = openai_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tlwr7YvplCTX"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.2, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFC8b-gbmHSb"
      },
      "source": [
        "DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0MFWF8lnpPd"
      },
      "outputs": [],
      "source": [
        "# Load the Excel file\n",
        "df1 = pd.ExcelFile('Data/Annotation_1.xlsx')\n",
        "\n",
        "# Get the sheet names\n",
        "sheet_names = df1.sheet_names\n",
        "\n",
        "# Create dataframes with specific names\n",
        "df_sentences = df1.parse(sheet_names[0])  # Assuming the first sheet contains sentences\n",
        "df_paragraphs = df1.parse(sheet_names[1])  # Assuming the second sheet contains paragraphs\n",
        "# Replace NaN values with 0 in df_sentences\n",
        "df_sentences.fillna(0, inplace=True)\n",
        "\n",
        "# Replace NaN values with 0 in df_paragraphs\n",
        "df_paragraphs.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8XcnnCLp8W-"
      },
      "outputs": [],
      "source": [
        "def response_to_dataframe(response):\n",
        "    extracted_data = []\n",
        "    lines = response.strip().split('\\n')  # Process all lines\n",
        "    for line in lines:\n",
        "        parts = line.split(':::', 1)\n",
        "        if len(parts) != 2:\n",
        "            print(f\"Skipped line (improper format): {line}\")  # Log the issue for visibility\n",
        "            continue\n",
        "        sentence, labels = parts\n",
        "        sentence = sentence.strip(' \"')\n",
        "        print(\"sentence     :\",sentence)\n",
        "        labels = labels.strip()\n",
        "        print(\"labels     :\",labels)\n",
        "        extracted_data.append((sentence, labels))\n",
        "    df = pd.DataFrame(extracted_data, columns=[\"sentence\", \"label\"])\n",
        "    return df\n",
        "\n",
        "final_df = pd.DataFrame(columns=[\"sentence\", \"label\", \"paragraph_id\"])  # Initialize an empty DataFrame\n",
        "\n",
        "# Initialize response_df here\n",
        "response_df = pd.DataFrame(columns=[\"response\", \"paragraph_id\"])\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    paragraph = row['Section']\n",
        "    paragraph_id = row['paragraph_ids']\n",
        "\n",
        "    # Construct the prompt\n",
        "    prompt = f\"\"\"\n",
        "    We are processing a regulatory document on food safety. The following concepts are of interest to us and ONLY these concepts should be used in your labels:\n",
        "    - Data: any information used to convey knowledge, provide assurance or perform analysis;\n",
        "    - Label Data: information that a food-product package or container must bear.\n",
        "    - Non-label Data: any food-safety-relevant data other than label data that needs to be collected and/or retained for inclusion in documents such as certificates, reports, guarantees and letters.\n",
        "    - Measurement: Association of numbers with physical quantities;\n",
        "    - Colour: (self-evident);\n",
        "    - Firmness: degree of resistance to deformation;\n",
        "    - Mass: amount of substance by weight or volume;\n",
        "    - Pathogen: a microorganism that causes disease;\n",
        "    - Size: dimension (e.g., length or thickness) or surface area.\n",
        "    - Temperature: (self-evident).\n",
        "    - Water Content: humidity or moisture.\n",
        "    - Time Constraint: A temporal restriction, in our context, is expressed using intervals, deadlines or periodicity.\n",
        "    Not all concepts are necessarily present. Extract relevant text segments and their associated concepts from the paragraph below and write nothing else!\n",
        "    The output should be in the following format: 'sentence ::: labels'.\n",
        "    Paragraph: ```{paragraph}```\n",
        "    \"\"\"\n",
        "    response = get_completion(prompt)\n",
        "    # Append the response and its associated paragraph_id to response_df\n",
        "    response_df.loc[len(response_df)] = [response, paragraph_id]\n",
        "    # print(\"row number\",index)\n",
        "    # print(response)\n",
        "\n",
        "    # Convert the response to a dataframe\n",
        "    current_df = response_to_dataframe(response)\n",
        "    current_df[\"paragraph_id\"] = paragraph_id  # Add the paragraph id\n",
        "\n",
        "    # Directly append to final_df\n",
        "    final_df = pd.concat([final_df, current_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plMh8QrsEuBq"
      },
      "source": [
        "1. Prepare the ground truth data.\n",
        "You have a ground truth DataFrame with columns \"sentence\", \"label\", and \"paragraph_id\".\n",
        "\n",
        "2. Iterate over each extracted sentence, find the best matching ground truth sentence, and compare labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_u9Feq2ewV4"
      },
      "outputs": [],
      "source": [
        "df_sentences = df_sentences.rename(columns={'LabelData': 'label data', 'Non-labelData': 'non-label data'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq1n1JjFdpmj"
      },
      "outputs": [],
      "source": [
        "# Convert the 'paragraph_id' column to int in both DataFrames\n",
        "final_df['paragraph_ids'] = final_df['paragraph_ids'].astype(int)\n",
        "df_sentences['paragraph_id'] = df_sentences['paragraph_id'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKazYiw_HYhx"
      },
      "outputs": [],
      "source": [
        "df_sentences.columns = [col.lower() for col in df_sentences.columns]\n",
        "labels_list = [\"Data\", \"Label Data\", \"Non-label Data\", \"Measurement\", \"Colour\", \"Firmness\", \"Mass\", \"Pathogen\", \"Size\", \"Temperature\", \"Water Content\", \"Time Constraint\",\"Overall\"]\n",
        "labels_list = [label.lower() for label in labels_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT7l4BgTImz0"
      },
      "outputs": [],
      "source": [
        "for label in labels_list:\n",
        "    if df_sentences[label].notna().all():  # Ensure no NaN values\n",
        "        df_sentences[label] = df_sentences[label].astype(int)\n",
        "    else:\n",
        "        print(f\"NaN values detected in '{label}' column!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdqBQwBV5ex0"
      },
      "outputs": [],
      "source": [
        "final_df['sentence'] = final_df['sentence'].str.replace(\"The output should be: \", \"\", regex=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaP5tQlHKm8T"
      },
      "outputs": [],
      "source": [
        "!pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2K4UrVjLEXn"
      },
      "outputs": [],
      "source": [
        "import Levenshtein"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfXJZKhxZ5px"
      },
      "source": [
        "A function to lowercase the first letter of string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcq3wn8qZ43B"
      },
      "outputs": [],
      "source": [
        "def lowercase_first_letter(s):\n",
        "    if not s:  # Check if the string is empty\n",
        "        return s\n",
        "    return s[0].lower() + s[1:]\n",
        "\n",
        "# Test\n",
        "string = \"Hello, World!\"\n",
        "result = lowercase_first_letter(string)\n",
        "print(result)  # Outputs: hello, World!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf1qxBLMLV6l"
      },
      "source": [
        "N-gram Jaccard similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLFncxtzLTF0"
      },
      "outputs": [],
      "source": [
        "def jaccard_similarity(list1, list2):\n",
        "    s1 = set(list1)\n",
        "    s2 = set(list2)\n",
        "    return len(s1.intersection(s2)) / len(s1.union(s2))\n",
        "\n",
        "def ngrams(input, n):\n",
        "    input = input.split(' ')\n",
        "    output = []\n",
        "    for i in range(len(input)-n+1):\n",
        "        output.append(' '.join(input[i:i+n]))\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDLT9zs5lwW3"
      },
      "outputs": [],
      "source": [
        "#This function will return the sentence from sentences_list that contains sentence as a substring and has the maximum length. If no such sentence is found, it will return None.\n",
        "def get_best_match(sentence, indexed_sentences_list,paragraph_id,Levenshtein_threshold,ngram_threshold,ngram_n=3):\n",
        "    # Strip single quotes from the beginning and end of the sentence\n",
        "    sentence=sentence.strip(\"'\\\"\")\n",
        "\n",
        "    if sentence.startswith('-'):\n",
        "      sentence = sentence[1:].lstrip()  # Remove the hyphen and any leading spaces\n",
        "      sentence = sentence.replace('\"', '', 1)  # Remove the first occurrence of \"\n",
        "\n",
        "    # Remove ;and or ;or from the end of the sentence\n",
        "    if sentence.endswith(';and'):\n",
        "        sentence = sentence[:-4]\n",
        "    elif sentence.endswith(';or'):\n",
        "        sentence = sentence[:-3]\n",
        "\n",
        "    # First, check for substring matches\n",
        "    for idx, s in indexed_sentences_list:\n",
        "        if sentence in s:\n",
        "            return idx,s\n",
        "\n",
        "    lowered_sentence=lowercase_first_letter(sentence)\n",
        "\n",
        "    for idx, s in indexed_sentences_list:\n",
        "      if lowered_sentence in s:\n",
        "          return idx,s\n",
        "\n",
        "    for idx, s in indexed_sentences_list:\n",
        "      if s in sentence:\n",
        "          # print(\"Gt substring of S\", s)\n",
        "          return idx,s\n",
        "\n",
        "    # If no exact substring match is found, check for similarity using Levenshtein distance\n",
        "    best_match = None\n",
        "    min_distance = float('inf')  # Initialize to a large value\n",
        "\n",
        "    best_Levenshtein_match_idx=0\n",
        "    for idx, s in indexed_sentences_list:\n",
        "        distance = Levenshtein.distance(lowered_sentence, s)\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            best_match = s\n",
        "            best_Levenshtein_match_idx=idx\n",
        "\n",
        "    # Convert the distance to a similarity ratio\n",
        "    similarity_ratio = 1 - min_distance / max(len(lowered_sentence), len(best_match))\n",
        "\n",
        "    if similarity_ratio > Levenshtein_threshold:\n",
        "        return best_Levenshtein_match_idx,best_match\n",
        "\n",
        "    elif similarity_ratio < Levenshtein_threshold:\n",
        "    # Check for substring matches\n",
        "      clean_sentence = ''.join(e for e in sentence if e.isalnum() or e.isspace()).strip()\n",
        "      for idx, s in indexed_sentences_list:\n",
        "          clean_s = ''.join(e for e in s if e.isalnum() or e.isspace()).strip()\n",
        "          if clean_sentence in clean_s:\n",
        "              return idx,s\n",
        "\n",
        "      best_ngram_similarity = 0\n",
        "      best_ngram_match = None\n",
        "      best_ngram_match_idx=0\n",
        "\n",
        "      sentence_ngrams = ngrams(sentence, ngram_n)\n",
        "      for idx, s in indexed_sentences_list:\n",
        "          s_ngrams = ngrams(s, ngram_n)\n",
        "          similarity = jaccard_similarity(sentence_ngrams, s_ngrams)\n",
        "\n",
        "          if similarity > best_ngram_similarity:\n",
        "              best_ngram_similarity = similarity\n",
        "              best_ngram_match = s\n",
        "              best_ngram_match_idx=idx\n",
        "\n",
        "      if best_ngram_similarity >= ngram_threshold:\n",
        "          return best_ngram_match_idx,best_ngram_match\n",
        "\n",
        "      print(f\"Paragraph ID: {paragraph_id}\")\n",
        "      print(f\"Unmatched sentence: {sentence}\")\n",
        "      print(f\"Best Levenshtein match with similarity ratio: {similarity_ratio} was: {best_match}\")\n",
        "      print(f\"Best n-gram match with similarity: {best_ngram_similarity} was: {best_ngram_match}\")\n",
        "      # print(f\"Best cosine match with similarity: {best_cosine_match} was: {best_cosine_match[0]}\")\n",
        "      print(\"-----\")\n",
        "\n",
        "      with open('output.txt', 'a') as file:\n",
        "        file.write(f\"Paragraph ID: {paragraph_id}\\n\")\n",
        "        file.write(f\"Unmatched sentence: {sentence}\\n\")\n",
        "        file.write(f\"Best match with similarity ratio: {similarity_ratio} was: {best_match}\\n\")\n",
        "        file.write(\"-----\\n\")\n",
        "      return None,None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grs4rhcriwzs"
      },
      "outputs": [],
      "source": [
        "counters = {label: {\"TP\": 0, \"FP\": 0, \"FN\": 0} for label in labels_list}\n",
        "\n",
        "# Dictionary to store missing labels for each matched ground truth sentence\n",
        "missing_labels_dict = {\n",
        "    (index, statement): [label for label in labels_list if df_sentences.loc[index, label] == 1]\n",
        "    for index, statement in df_sentences['statement'].items()\n",
        "}\n",
        "\n",
        "for label in labels_list:\n",
        "    init_count = sum([1 for labels in missing_labels_dict.values() if label in labels])\n",
        "    assert init_count == df_sentences[label].sum(), f\"Mismatch for {label} during initialization\"\n",
        "\n",
        "for index, row in final_df.iterrows():\n",
        "    extracted_sentence = row['sentence']\n",
        "    paragraph_id = row['paragraph_ids']\n",
        "\n",
        "    # Filter the ground truth dataframe by the current paragraph_id\n",
        "    filtered_gt_df = df_sentences[df_sentences['paragraph_id'] == paragraph_id]\n",
        "\n",
        "    # Prepare a list of tuples (index, statement)\n",
        "    indexed_statements = list(filtered_gt_df[['statement']].itertuples(index=True, name=None))\n",
        "\n",
        "    # Find the best matching ground truth sentence and its index\n",
        "    matched_index, matched_sentence = get_best_match(extracted_sentence, indexed_statements, paragraph_id,Levenshtein_threshold=0.90,ngram_threshold=0.90)\n",
        "\n",
        "    # If there's a match, compare each label\n",
        "    # if matched_index and matched_sentence:\n",
        "\n",
        "    if matched_index is not None and matched_sentence is not None:\n",
        "\n",
        "\n",
        "        ground_truth_row = filtered_gt_df.loc[filtered_gt_df['statement'] == matched_sentence].iloc[0]\n",
        "\n",
        "        for label in labels_list:\n",
        "            # Extracted is 1, Ground Truth is 1: True Positive\n",
        "            if row[label] == 1 and ground_truth_row[label] == 1:\n",
        "                if label in missing_labels_dict[matched_index,matched_sentence]:  # Only if the label is still missing\n",
        "                  counters[label][\"TP\"] += 1\n",
        "                  missing_labels_dict[(matched_index, matched_sentence)].remove(label)  # Remove the label as it's no longer missing\n",
        "\n",
        "            # Extracted is 1, Ground Truth is 0: False Positive\n",
        "            elif row[label] == 1 and ground_truth_row[label] == 0:\n",
        "                counters[label][\"FP\"] += 1\n",
        "\n",
        "\n",
        "    else:\n",
        "\n",
        "        for label in labels_list:\n",
        "            if row[label] == 1:\n",
        "                counters[label][\"FP\"] += 1\n",
        "\n",
        "# FN count is incremented for missing labels in the gt sentences.\n",
        "for sentence, missing_labels in missing_labels_dict.items():\n",
        "    for label in missing_labels:\n",
        "        counters[label][\"FN\"] += 1\n",
        "\n",
        "# Compute precision, recall, and F-score for each label\n",
        "def compute_scores(tp, fp, fn):\n",
        "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "    fscore = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "    return precision, recall, fscore\n",
        "\n",
        "scores = {}\n",
        "for label, counts in counters.items():\n",
        "    precision, recall, fscore = compute_scores(counts[\"TP\"], counts[\"FP\"], counts[\"FN\"])\n",
        "    scores[label] = {\"Precision\": precision, \"Recall\": recall, \"F-score\": fscore}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl72NZJwu4Xf"
      },
      "outputs": [],
      "source": [
        "with open('output.txt', 'a') as f:\n",
        "    f.write(str(counters))\n",
        "    f.write('\\n\\n')  # Separate the two dictionaries with two newlines\n",
        "    f.write(str(scores))\n",
        "    f.write('\\n\\n')  # Separate the two dictionaries with two newlines\n",
        "    f.write(\"-----\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cte_2Uf1albq"
      },
      "outputs": [],
      "source": [
        "for (idx, sentence), labels in missing_labels_dict.items():\n",
        "    if not labels:  # if the list of labels is empty\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Retrieve the paragraph_id from df_sentences using the index\n",
        "        paragraph_id = df_sentences.loc[idx, 'paragraph_id']\n",
        "    except KeyError:\n",
        "        print(f\"ERROR: Index not found in df_sentences: {idx}\")\n",
        "        continue\n",
        "\n",
        "    with open('output.txt', 'a') as f:\n",
        "        f.write(f\"Paragraph ID: {paragraph_id}\\n\")\n",
        "        f.write(f\"Sentence: {sentence}\\n\")\n",
        "        f.write(f\"Missing Labels: {labels}\\n\")\n",
        "        f.write(\"-----\\n\")\n",
        "\n",
        "    print(f\"Paragraph ID: {paragraph_id}\")\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Missing Labels: {labels}\")\n",
        "    print(\"-----\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHS9ZNdjpzc-"
      },
      "outputs": [],
      "source": [
        "# Get the number of missing labels for each sentence\n",
        "missing_label_counts = [len(labels) for labels in missing_labels_dict.values()]\n",
        "\n",
        "# Get the range and some basic statistics\n",
        "min_missing = min(missing_label_counts)\n",
        "max_missing = max(missing_label_counts)\n",
        "avg_missing = sum(missing_label_counts) / len(missing_label_counts)\n",
        "\n",
        "print(f\"Range of missing labels: {min_missing} to {max_missing}\")\n",
        "print(f\"Average missing labels per sentence: {avg_missing:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoGmpO7QM3ZO"
      },
      "source": [
        "####Test the created Response and final_df files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cT4e_P3iYdiH"
      },
      "outputs": [],
      "source": [
        "# Read the Response.txt file as plain text and split using \"++++++\"\n",
        "with open('Resposne.txt', 'r') as file:\n",
        "    content_response = file.read()\n",
        "    responses = content_response.split('++++++')\n",
        "\n",
        "# Clean up any leading/trailing whitespace from each response\n",
        "responses = [response.strip() for response in responses]\n",
        "response_df = pd.DataFrame(responses, columns=['Response'])\n",
        "\n",
        "# Read the paragraph_id.txt file as plain text and split using \"++++++\"\n",
        "with open('paragraphid.txt', 'r') as file:\n",
        "    content_paragraph_id = file.read()\n",
        "    paragraph_ids = content_paragraph_id.split('++++++')\n",
        "\n",
        "# Clean up any leading/trailing whitespace from each ID\n",
        "paragraph_ids = [pid.strip() for pid in paragraph_ids]\n",
        "paragraph_id_df = pd.DataFrame(paragraph_ids, columns=['Paragraph_ID'])\n",
        "\n",
        "# Combine both DataFrames side by side\n",
        "combined_df = pd.concat([response_df, paragraph_id_df], axis=1)\n",
        "\n",
        "print(combined_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EQ1NDGFXzQa"
      },
      "outputs": [],
      "source": [
        "def response_to_dataframe(df_input):\n",
        "    extracted_data = []\n",
        "\n",
        "    for _, row in df_input.iterrows():\n",
        "        response = row['Response']\n",
        "        paragraph_id = row['Paragraph_ID']\n",
        "\n",
        "        lines = response.strip().split('\\n')  # Process all lines\n",
        "        for line in lines:\n",
        "            if ':::' in line:\n",
        "                parts = line.strip().split(':::')\n",
        "                if len(parts) != 2:\n",
        "                    print(f\"Skipped line (improper format): {line}\")  # Log the issue for visibility\n",
        "                    continue\n",
        "                sentence, labels = parts\n",
        "                labels = labels.strip()\n",
        "                extracted_data.append((sentence, labels, paragraph_id))\n",
        "            else:\n",
        "                print(f\"Skipped line (improper format): {line}\")  # Log the issue for visibility\n",
        "                continue\n",
        "\n",
        "    df = pd.DataFrame(extracted_data, columns=[\"sentence\", \"labels\", \"paragraph_ids\"])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGmbgwY4aN2t"
      },
      "outputs": [],
      "source": [
        "final_df=response_to_dataframe(combined_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq2IoYMXtyyQ"
      },
      "source": [
        "Read data from output final_df and create its dataframe here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG384bevySxJ"
      },
      "outputs": [],
      "source": [
        "# Read the paragraph d of the sentences\n",
        "with open('final_df_paragraphid.txt', 'r') as file:\n",
        "    paragraph_ids_content = file.read()\n",
        "\n",
        "# Read the content of the sentences file\n",
        "with open('final_df.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Split the content by the delimiter and strip each sentence of leading/trailing whitespaces and newlines\n",
        "sentences = [s.strip() for s in content.strip().split('++++++\\n')]\n",
        "\n",
        "# Create a dataframe from the list of sentences\n",
        "final_df = pd.DataFrame(sentences, columns=['sentence'])\n",
        "\n",
        "# Read the content of the labels file\n",
        "with open('final_df_labels.txt', 'r') as file:\n",
        "    labels_content = file.read()\n",
        "\n",
        "# Split the content by the delimiter and strip each label of leading/trailing whitespaces and newlines\n",
        "labels = [l.strip() for l in labels_content.strip().split('++++++\\n')]\n",
        "\n",
        "# Add the labels as a new column to the dataframe\n",
        "final_df['labels'] = labels\n",
        "\n",
        "\n",
        "# Split the content by the delimiter\n",
        "paragraph_ids = [p.strip() for p in paragraph_ids_content.strip().split('++++++\\n')]\n",
        "\n",
        "# Add the labels as a new column to the dataframe\n",
        "final_df['paragraph_ids'] = paragraph_ids\n",
        "\n",
        "\n",
        "# Check if the lengths are consistent\n",
        "if not (len(sentences) == len(labels) == len(paragraph_ids)):\n",
        "    raise ValueError(\"The files have inconsistent numbers of rows\")\n",
        "\n",
        "# Create a dataframe from the lists\n",
        "final_df = pd.DataFrame({\n",
        "    'sentence': sentences,\n",
        "    'labels': labels,\n",
        "    'paragraph_ids': paragraph_ids\n",
        "})\n",
        "\n",
        "# Clean up the last row if needed\n",
        "final_df['labels'] = final_df['labels'].str.replace('\\n++++++', '', regex=False)\n",
        "final_df['paragraph_ids'] = final_df['paragraph_ids'].str.replace('\\n++++++', '', regex=False)\n",
        "\n",
        "# Display the dataframe\n",
        "final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4v-VNgCV1xeP"
      },
      "outputs": [],
      "source": [
        "# Replace unwanted characters in the 'labels' column and convert to lowercase\n",
        "final_df['labels'] = final_df['labels'].str.replace(\"\"\"[.'\"]\"\"\", \"\", regex=True).str.strip().str.lower()\n",
        "\n",
        "# Removing extra spaces after the commas\n",
        "final_df['labels'] = final_df['labels'].str.replace(\", \", \",\", regex=False)\n",
        "\n",
        "# Display the cleaned dataframe\n",
        "final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujOgCjUl3-L2"
      },
      "outputs": [],
      "source": [
        "# Filter rows where the label, after being stripped of whitespace, is empty\n",
        "only_whitespace_rows = final_df[final_df['labels'].str.strip() == \"\"]\n",
        "\n",
        "# Display the index, sentence, and label columns of these rows\n",
        "print(only_whitespace_rows[['sentence', 'labels']])\n",
        "\n",
        "# Get the indices of these rows from the main dataframe\n",
        "row_indices = only_whitespace_rows.index.tolist()\n",
        "\n",
        "print(row_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rGNWkynL7Qk"
      },
      "outputs": [],
      "source": [
        "labels_list = [\"Data\", \"Label Data\", \"Non-label Data\", \"Measurement\", \"Colour\", \"Firmness\", \"Mass\", \"Pathogen\", \"Size\", \"Temperature\", \"Water Content\", \"Time Constraint\"]\n",
        "\n",
        "# Convert the labels list to lowercase for case-insensitivity (optional)\n",
        "labels_list = [label.lower() for label in labels_list]\n",
        "\n",
        "# Initialize all label columns with 0\n",
        "for label in labels_list:\n",
        "    final_df[label] = 0\n",
        "\n",
        "\n",
        "for index, row in final_df.iterrows():\n",
        "    # Split the labels while preserving content within parentheses\n",
        "    labels = [label.strip().lower() for label in re.split(r',(?![^\\(]*\\))', row['labels'])]\n",
        "\n",
        "    for label in labels:\n",
        "        if label in labels_list:\n",
        "            final_df.at[index, label] = 1\n",
        "\n",
        "\n",
        "        elif '(' in label:\n",
        "            # Check for main label\n",
        "            main_label = re.split(r'[(]', label)[0].strip()\n",
        "            if not main_label:\n",
        "              print(\"cases like (...:...) happend\", label, index)\n",
        "\n",
        "            if main_label and main_label in labels_list:\n",
        "                final_df.at[index, main_label] = 1\n",
        "\n",
        "            # Extract subtypes within parentheses\n",
        "            sub_labels = re.findall(r'\\((.*?)\\)', label)\n",
        "            for sub_group in sub_labels:\n",
        "                # Split by colon if present\n",
        "                subtypes = [sub.strip() for sub in sub_group.split(',') if sub]\n",
        "                for subtype in subtypes:\n",
        "                    if subtype in labels_list:\n",
        "                        final_df.at[index, subtype] = 1\n",
        "                    else:\n",
        "                        # Split by spaces to check individual words\n",
        "                        for word in subtype.split():\n",
        "                            if word in labels_list:\n",
        "                                final_df.at[index, word] = 1\n",
        "                            else:\n",
        "                                print(\"sub word in the () not matched: \",word ,index)\n",
        "        elif ':' in label:\n",
        "          main_label = re.split(r'[:]', label)[0].strip()\n",
        "          sub_label = re.split(r'[:]', label)[1].strip()\n",
        "\n",
        "          if main_label in labels_list:\n",
        "            final_df.at[index, main_label] = 1\n",
        "          if sub_label in labels_list:\n",
        "            final_df.at[index, sub_label] = 1\n",
        "        else:\n",
        "            print(\"label not matched: \",label, \"at the index: \",index)\n",
        "\n",
        "final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZGjVo3dg9HU"
      },
      "outputs": [],
      "source": [
        "final_df['overall'] = 0\n",
        "\n",
        "labels_list = [\"Data\", \"Label Data\", \"Non-label Data\", \"Measurement\", \"Colour\", \"Firmness\", \"Mass\", \"Pathogen\", \"Size\", \"Temperature\", \"Water Content\", \"Time Constraint\"]\n",
        "measurement_subtypes = [\"Colour\", \"Firmness\", \"Mass\", \"Size\", \"Temperature\", \"Water Content\",\"Pathogen\"]\n",
        "data_subtypes = [\"Label Data\", \"Non-label Data\"]\n",
        "\n",
        "\n",
        "# Convert the labels list to lowercase for case-insensitivity (optional)\n",
        "labels_list = [label.lower() for label in labels_list]\n",
        "measurement_subtypes= [label.lower() for label in measurement_subtypes]\n",
        "data_subtypes= [label.lower() for label in data_subtypes]\n",
        "\n",
        "for index, row in final_df.iterrows():\n",
        "    # Set 'Overall' to 1 if any label is found\n",
        "    if any(row[label] == 1 for label in labels_list):\n",
        "        final_df.at[index, 'overall'] = 1\n",
        "\n",
        "    # Set 'Measurement' to 1 if any of its subtypes are found\n",
        "    if any(row[subtype] == 1 for subtype in measurement_subtypes):\n",
        "        final_df.at[index, 'measurement'] = 1\n",
        "\n",
        "    # Set 'Data' to 1 if any of its subtypes are found\n",
        "    if any(row[subtype] == 1 for subtype in data_subtypes):\n",
        "        final_df.at[index, 'data'] = 1\n",
        "\n",
        "final_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Ig9VKDucS8"
      },
      "source": [
        "#Analysis of Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeEhZXVTkCTe"
      },
      "source": [
        "Zero shot Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQBT-izVubnh"
      },
      "outputs": [],
      "source": [
        "experiments_data = []\n",
        "experiments_data.append({'data': {'Precision': 0.19672131147540983, 'Recall': 0.8450704225352113, 'F-score': 0.3191489361702128}, 'label data': {'Precision': 0.2556818181818182, 'Recall': 0.8333333333333334, 'F-score': 0.3913043478260869}, 'non-label data': {'Precision': 0.1308411214953271, 'Recall': 0.7368421052631579, 'F-score': 0.22222222222222218}, 'measurement': {'Precision': 0.5979381443298969, 'Recall': 0.4027777777777778, 'F-score': 0.48132780082987553}, 'colour': {'Precision': 0.38461538461538464, 'Recall': 0.4166666666666667, 'F-score': 0.4}, 'firmness': {'Precision': 0.16666666666666666, 'Recall': 0.5, 'F-score': 0.25}, 'mass': {'Precision': 0.6923076923076923, 'Recall': 0.19148936170212766, 'F-score': 0.30000000000000004}, 'pathogen': {'Precision': 0.18181818181818182, 'Recall': 1.0, 'F-score': 0.3076923076923077}, 'size': {'Precision': 0.3333333333333333, 'Recall': 0.2777777777777778, 'F-score': 0.303030303030303}, 'temperature': {'Precision': 0.8235294117647058, 'Recall': 1.0, 'F-score': 0.9032258064516129}, 'water content': {'Precision': 0.4, 'Recall': 0.5, 'F-score': 0.4444444444444445}, 'time constraint': {'Precision': 0.5, 'Recall': 0.17647058823529413, 'F-score': 0.2608695652173913}, 'overall': {'Precision': 0.4968152866242038, 'Recall': 0.7428571428571429, 'F-score': 0.5954198473282444}})\n",
        "experiments_data.append({'data': {'Precision': 0.21140939597315436, 'Recall': 0.8873239436619719, 'F-score': 0.34146341463414637}, 'label data': {'Precision': 0.25842696629213485, 'Recall': 0.8518518518518519, 'F-score': 0.3965517241379311}, 'non-label data': {'Precision': 0.16483516483516483, 'Recall': 0.7894736842105263, 'F-score': 0.2727272727272727}, 'measurement': {'Precision': 0.5729166666666666, 'Recall': 0.3819444444444444, 'F-score': 0.4583333333333333}, 'colour': {'Precision': 0.4166666666666667, 'Recall': 0.4166666666666667, 'F-score': 0.4166666666666667}, 'firmness': {'Precision': 0.16666666666666666, 'Recall': 0.5, 'F-score': 0.25}, 'mass': {'Precision': 0.7272727272727273, 'Recall': 0.1702127659574468, 'F-score': 0.27586206896551724}, 'pathogen': {'Precision': 0.19047619047619047, 'Recall': 1.0, 'F-score': 0.32}, 'size': {'Precision': 0.2903225806451613, 'Recall': 0.25, 'F-score': 0.2686567164179105}, 'temperature': {'Precision': 0.875, 'Recall': 1.0, 'F-score': 0.9333333333333333}, 'water content': {'Precision': 1.0, 'Recall': 0.5, 'F-score': 0.6666666666666666}, 'time constraint': {'Precision': 0.6, 'Recall': 0.17647058823529413, 'F-score': 0.2727272727272727}, 'overall': {'Precision': 0.5031847133757962, 'Recall': 0.7523809523809524, 'F-score': 0.6030534351145038}})\n",
        "experiments_data.append({'data': {'Precision': 0.17717717717717718, 'Recall': 0.8309859154929577, 'F-score': 0.2920792079207921}, 'label data': {'Precision': 0.225, 'Recall': 0.8333333333333334, 'F-score': 0.3543307086614173}, 'non-label data': {'Precision': 0.11304347826086956, 'Recall': 0.6842105263157895, 'F-score': 0.19402985074626866}, 'measurement': {'Precision': 0.5754716981132075, 'Recall': 0.4236111111111111, 'F-score': 0.488}, 'colour': {'Precision': 0.35714285714285715, 'Recall': 0.4166666666666667, 'F-score': 0.3846153846153846}, 'firmness': {'Precision': 0.2, 'Recall': 0.5, 'F-score': 0.28571428571428575}, 'mass': {'Precision': 0.8076923076923077, 'Recall': 0.22340425531914893, 'F-score': 0.35}, 'pathogen': {'Precision': 0.2, 'Recall': 1.0, 'F-score': 0.33333333333333337}, 'size': {'Precision': 0.30303030303030304, 'Recall': 0.2777777777777778, 'F-score': 0.28985507246376807}, 'temperature': {'Precision': 0.875, 'Recall': 1.0, 'F-score': 0.9333333333333333}, 'water content': {'Precision': 1.0, 'Recall': 0.5, 'F-score': 0.6666666666666666}, 'time constraint': {'Precision': 0.6, 'Recall': 0.17647058823529413, 'F-score': 0.2727272727272727}, 'overall': {'Precision': 0.46647230320699706, 'Recall': 0.7619047619047619, 'F-score': 0.5786618444846292}})\n",
        "experiments_data.append({'data': {'Precision': 0.19135802469135801, 'Recall': 0.8732394366197183, 'F-score': 0.31392405063291134}, 'label data': {'Precision': 0.24210526315789474, 'Recall': 0.8518518518518519, 'F-score': 0.3770491803278688}, 'non-label data': {'Precision': 0.11711711711711711, 'Recall': 0.6842105263157895, 'F-score': 0.19999999999999998}, 'measurement': {'Precision': 0.5981308411214953, 'Recall': 0.4444444444444444, 'F-score': 0.5099601593625499}, 'colour': {'Precision': 0.45454545454545453, 'Recall': 0.4166666666666667, 'F-score': 0.43478260869565216}, 'firmness': {'Precision': 0.16666666666666666, 'Recall': 0.5, 'F-score': 0.25}, 'mass': {'Precision': 0.8636363636363636, 'Recall': 0.20212765957446807, 'F-score': 0.3275862068965517}, 'pathogen': {'Precision': 0.2, 'Recall': 1.0, 'F-score': 0.33333333333333337}, 'size': {'Precision': 0.34285714285714286, 'Recall': 0.3333333333333333, 'F-score': 0.3380281690140845}, 'temperature': {'Precision': 0.8235294117647058, 'Recall': 1.0, 'F-score': 0.9032258064516129}, 'water content': {'Precision': 0.75, 'Recall': 0.75, 'F-score': 0.75}, 'time constraint': {'Precision': 0.6, 'Recall': 0.17647058823529413, 'F-score': 0.2727272727272727}, 'overall': {'Precision': 0.4864864864864865, 'Recall': 0.7714285714285715, 'F-score': 0.5966850828729282}})\n",
        "experiments_data.append({'data': {'Precision': 0.20748299319727892, 'Recall': 0.8591549295774648, 'F-score': 0.3342465753424657}, 'label data': {'Precision': 0.24581005586592178, 'Recall': 0.8148148148148148, 'F-score': 0.37768240343347637}, 'non-label data': {'Precision': 0.14432989690721648, 'Recall': 0.7368421052631579, 'F-score': 0.24137931034482754}, 'measurement': {'Precision': 0.6039603960396039, 'Recall': 0.4236111111111111, 'F-score': 0.4979591836734694}, 'colour': {'Precision': 0.4166666666666667, 'Recall': 0.4166666666666667, 'F-score': 0.4166666666666667}, 'firmness': {'Precision': 0.25, 'Recall': 0.5, 'F-score': 0.3333333333333333}, 'mass': {'Precision': 0.6818181818181818, 'Recall': 0.1595744680851064, 'F-score': 0.25862068965517243}, 'pathogen': {'Precision': 0.2222222222222222, 'Recall': 1.0, 'F-score': 0.3636363636363636}, 'size': {'Precision': 0.35714285714285715, 'Recall': 0.2777777777777778, 'F-score': 0.31250000000000006}, 'temperature': {'Precision': 0.875, 'Recall': 1.0, 'F-score': 0.9333333333333333}, 'water content': {'Precision': 0.5, 'Recall': 0.5, 'F-score': 0.5}, 'time constraint': {'Precision': 0.625, 'Recall': 0.29411764705882354, 'F-score': 0.4}, 'overall': {'Precision': 0.511326860841424, 'Recall': 0.7523809523809524, 'F-score': 0.6088631984585743}})\n",
        "experiments_data.append({'data': {'Precision': 0.2026578073089701, 'Recall': 0.8591549295774648, 'F-score': 0.32795698924731187}, 'label data': {'Precision': 0.24193548387096775, 'Recall': 0.8333333333333334, 'F-score': 0.37500000000000006}, 'non-label data': {'Precision': 0.13725490196078433, 'Recall': 0.7368421052631579, 'F-score': 0.23140495867768596}, 'measurement': {'Precision': 0.5523809523809524, 'Recall': 0.4027777777777778, 'F-score': 0.46586345381526106}, 'colour': {'Precision': 0.45454545454545453, 'Recall': 0.4166666666666667, 'F-score': 0.43478260869565216}, 'firmness': {'Precision': 0.2, 'Recall': 0.5, 'F-score': 0.28571428571428575}, 'mass': {'Precision': 0.9047619047619048, 'Recall': 0.20212765957446807, 'F-score': 0.3304347826086957}, 'pathogen': {'Precision': 0.18181818181818182, 'Recall': 1.0, 'F-score': 0.3076923076923077}, 'size': {'Precision': 0.26666666666666666, 'Recall': 0.2222222222222222, 'F-score': 0.2424242424242424}, 'temperature': {'Precision': 0.8235294117647058, 'Recall': 1.0, 'F-score': 0.9032258064516129}, 'water content': {'Precision': 0.75, 'Recall': 0.75, 'F-score': 0.75}, 'time constraint': {'Precision': 0.6, 'Recall': 0.17647058823529413, 'F-score': 0.2727272727272727}, 'overall': {'Precision': 0.5032051282051282, 'Recall': 0.7476190476190476, 'F-score': 0.6015325670498084}})\n",
        "experiments_data.append({'data': {'Precision': 0.18333333333333332, 'Recall': 0.9295774647887324, 'F-score': 0.30626450116009285}, 'label data': {'Precision': 0.24873096446700507, 'Recall': 0.9074074074074074, 'F-score': 0.39043824701195223}, 'non-label data': {'Precision': 0.09395973154362416, 'Recall': 0.7368421052631579, 'F-score': 0.16666666666666669}, 'measurement': {'Precision': 0.6052631578947368, 'Recall': 0.4791666666666667, 'F-score': 0.5348837209302326}, 'colour': {'Precision': 0.5, 'Recall': 0.5, 'F-score': 0.5}, 'firmness': {'Precision': 0.0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.8260869565217391, 'Recall': 0.20212765957446807, 'F-score': 0.3247863247863248}, 'pathogen': {'Precision': 0.23529411764705882, 'Recall': 1.0, 'F-score': 0.38095238095238093}, 'size': {'Precision': 0.425, 'Recall': 0.4722222222222222, 'F-score': 0.4473684210526316}, 'temperature': {'Precision': 0.8235294117647058, 'Recall': 1.0, 'F-score': 0.9032258064516129}, 'water content': {'Precision': 0.5, 'Recall': 0.5, 'F-score': 0.5}, 'time constraint': {'Precision': 0.4, 'Recall': 0.11764705882352941, 'F-score': 0.1818181818181818}, 'overall': {'Precision': 0.49866666666666665, 'Recall': 0.8904761904761904, 'F-score': 0.6393162393162393}})\n",
        "experiments_data.append({'data': {'Precision': 0.17366946778711484, 'Recall': 0.8732394366197183, 'F-score': 0.2897196261682243}, 'label data': {'Precision': 0.22727272727272727, 'Recall': 0.8333333333333334, 'F-score': 0.35714285714285715}, 'non-label data': {'Precision': 0.10344827586206896, 'Recall': 0.7894736842105263, 'F-score': 0.18292682926829268}, 'measurement': {'Precision': 0.5779816513761468, 'Recall': 0.4375, 'F-score': 0.4980237154150198}, 'colour': {'Precision': 0.3076923076923077, 'Recall': 0.3333333333333333, 'F-score': 0.32}, 'firmness': {'Precision': 0.0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.85, 'Recall': 0.18085106382978725, 'F-score': 0.29824561403508776}, 'pathogen': {'Precision': 0.19047619047619047, 'Recall': 1.0, 'F-score': 0.32}, 'size': {'Precision': 0.38461538461538464, 'Recall': 0.4166666666666667, 'F-score': 0.4}, 'temperature': {'Precision': 0.8235294117647058, 'Recall': 1.0, 'F-score': 0.9032258064516129}, 'water content': {'Precision': 1.0, 'Recall': 0.5, 'F-score': 0.6666666666666666}, 'time constraint': {'Precision': 0.3333333333333333, 'Recall': 0.11764705882352941, 'F-score': 0.1739130434782609}, 'overall': {'Precision': 0.4540540540540541, 'Recall': 0.8, 'F-score': 0.5793103448275863}})\n",
        "experiments_data.append({'data': {'Precision': 0.18155619596541786, 'Recall': 0.8873239436619719, 'F-score': 0.3014354066985646}, 'label data': {'Precision': 0.25, 'Recall': 0.8333333333333334, 'F-score': 0.3846153846153846}, 'non-label data': {'Precision': 0.09375, 'Recall': 0.7894736842105263, 'F-score': 0.16759776536312848}, 'measurement': {'Precision': 0.6272727272727273, 'Recall': 0.4791666666666667, 'F-score': 0.5433070866141733}, 'colour': {'Precision': 0.5, 'Recall': 0.6666666666666666, 'F-score': 0.5714285714285715}, 'firmness': {'Precision': 0.0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.8095238095238095, 'Recall': 0.18085106382978725, 'F-score': 0.2956521739130435}, 'pathogen': {'Precision': 0.2, 'Recall': 1.0, 'F-score': 0.33333333333333337}, 'size': {'Precision': 0.4722222222222222, 'Recall': 0.4722222222222222, 'F-score': 0.4722222222222222}, 'temperature': {'Precision': 0.8235294117647058, 'Recall': 1.0, 'F-score': 0.9032258064516129}, 'water content': {'Precision': 0.6666666666666666, 'Recall': 0.5, 'F-score': 0.5714285714285715}, 'time constraint': {'Precision': 0.3333333333333333, 'Recall': 0.11764705882352941, 'F-score': 0.1739130434782609}, 'overall': {'Precision': 0.4891304347826087, 'Recall': 0.8571428571428571, 'F-score': 0.6228373702422144}})\n",
        "experiments_data.append({'data': {'Precision': 0.1745152354570637, 'Recall': 0.8873239436619719, 'F-score': 0.2916666666666667}, 'label data': {'Precision': 0.23115577889447236, 'Recall': 0.8518518518518519, 'F-score': 0.3636363636363636}, 'non-label data': {'Precision': 0.08, 'Recall': 0.631578947368421, 'F-score': 0.14201183431952663}, 'measurement': {'Precision': 0.6339285714285714, 'Recall': 0.4930555555555556, 'F-score': 0.5546875}, 'colour': {'Precision': 0.4375, 'Recall': 0.5833333333333334, 'F-score': 0.5}, 'firmness': {'Precision': 0.0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.875, 'Recall': 0.22340425531914893, 'F-score': 0.3559322033898305}, 'pathogen': {'Precision': 0.2, 'Recall': 1.0, 'F-score': 0.33333333333333337}, 'size': {'Precision': 0.42105263157894735, 'Recall': 0.4444444444444444, 'F-score': 0.43243243243243246}, 'temperature': {'Precision': 0.875, 'Recall': 1.0, 'F-score': 0.9333333333333333}, 'water content': {'Precision': 1.0, 'Recall': 0.5, 'F-score': 0.6666666666666666}, 'time constraint': {'Precision': 0.3333333333333333, 'Recall': 0.11764705882352941, 'F-score': 0.1739130434782609}, 'overall': {'Precision': 0.4854111405835544, 'Recall': 0.8714285714285714, 'F-score': 0.6235093696763203}})\n",
        "experiments_data.append({'data': {'Precision': 0.1867816091954023, 'Recall': 0.9154929577464789, 'F-score': 0.31026252983293556}, 'label data': {'Precision': 0.28160919540229884, 'Recall': 0.9074074074074074, 'F-score': 0.4298245614035088}, 'non-label data': {'Precision': 0.09210526315789473, 'Recall': 0.7368421052631579, 'F-score': 0.16374269005847952}, 'measurement': {'Precision': 0.625, 'Recall': 0.4861111111111111, 'F-score': 0.5468749999999999}, 'colour': {'Precision': 0.5294117647058824, 'Recall': 0.75, 'F-score': 0.6206896551724139}, 'firmness': {'Precision': 0.25, 'Recall': 0.5, 'F-score': 0.3333333333333333}, 'mass': {'Precision': 0.75, 'Recall': 0.19148936170212766, 'F-score': 0.3050847457627119}, 'pathogen': {'Precision': 0.21052631578947367, 'Recall': 1.0, 'F-score': 0.34782608695652173}, 'size': {'Precision': 0.4594594594594595, 'Recall': 0.4722222222222222, 'F-score': 0.4657534246575342}, 'temperature': {'Precision': 0.7368421052631579, 'Recall': 1.0, 'F-score': 0.8484848484848484}, 'water content': {'Precision': 1.0, 'Recall': 0.5, 'F-score': 0.6666666666666666}, 'time constraint': {'Precision': 0.3333333333333333, 'Recall': 0.11764705882352941, 'F-score': 0.1739130434782609}, 'overall': {'Precision': 0.5027472527472527, 'Recall': 0.8714285714285714, 'F-score': 0.6376306620209059}})\n",
        "experiments_data.append({'data': {'Precision': 0.18285714285714286, 'Recall': 0.9014084507042254, 'F-score': 0.30403800475059384}, 'label data': {'Precision': 0.25, 'Recall': 0.8888888888888888, 'F-score': 0.3902439024390244}, 'non-label data': {'Precision': 0.09090909090909091, 'Recall': 0.6842105263157895, 'F-score': 0.16049382716049382}, 'measurement': {'Precision': 0.5862068965517241, 'Recall': 0.4722222222222222, 'F-score': 0.523076923076923}, 'colour': {'Precision': 0.47058823529411764, 'Recall': 0.6666666666666666, 'F-score': 0.5517241379310345}, 'firmness': {'Precision': 0.0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.875, 'Recall': 0.22340425531914893, 'F-score': 0.3559322033898305}, 'pathogen': {'Precision': 0.25, 'Recall': 1.0, 'F-score': 0.4}, 'size': {'Precision': 0.3170731707317073, 'Recall': 0.3611111111111111, 'F-score': 0.33766233766233766}, 'temperature': {'Precision': 0.8235294117647058, 'Recall': 1.0, 'F-score': 0.9032258064516129}, 'water content': {'Precision': 1.0, 'Recall': 0.5, 'F-score': 0.6666666666666666}, 'time constraint': {'Precision': 0.2, 'Recall': 0.058823529411764705, 'F-score': 0.0909090909090909}, 'overall': {'Precision': 0.4745308310991957, 'Recall': 0.8428571428571429, 'F-score': 0.6072041166380789}})\n",
        "experiments_data.append({'data': {'Precision': 0.1763085399449036, 'Recall': 0.9014084507042254, 'F-score': 0.2949308755760368}, 'label data': {'Precision': 0.23711340206185566, 'Recall': 0.8518518518518519, 'F-score': 0.3709677419354838}, 'non-label data': {'Precision': 0.08024691358024691, 'Recall': 0.6842105263157895, 'F-score': 0.143646408839779}, 'measurement': {'Precision': 0.6126126126126126, 'Recall': 0.4722222222222222, 'F-score': 0.5333333333333332}, 'colour': {'Precision': 0.5333333333333333, 'Recall': 0.6666666666666666, 'F-score': 0.5925925925925926}, 'firmness': {'Precision': 0.0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.8571428571428571, 'Recall': 0.19148936170212766, 'F-score': 0.31304347826086953}, 'pathogen': {'Precision': 0.23529411764705882, 'Recall': 1.0, 'F-score': 0.38095238095238093}, 'size': {'Precision': 0.47368421052631576, 'Recall': 0.5, 'F-score': 0.4864864864864865}, 'temperature': {'Precision': 0.8235294117647058, 'Recall': 1.0, 'F-score': 0.9032258064516129}, 'water content': {'Precision': 0.6666666666666666, 'Recall': 0.5, 'F-score': 0.5714285714285715}, 'time constraint': {'Precision': 0.42857142857142855, 'Recall': 0.17647058823529413, 'F-score': 0.25}, 'overall': {'Precision': 0.48021108179419525, 'Recall': 0.8666666666666667, 'F-score': 0.6179966044142615}})\n",
        "experiments_data.append({'data': {'Precision': 0.1787709497206704, 'Recall': 0.9014084507042254, 'F-score': 0.2983682983682984}, 'label data': {'Precision': 0.26229508196721313, 'Recall': 0.8888888888888888, 'F-score': 0.4050632911392405}, 'non-label data': {'Precision': 0.08024691358024691, 'Recall': 0.6842105263157895, 'F-score': 0.143646408839779}, 'measurement': {'Precision': 0.6637931034482759, 'Recall': 0.5347222222222222, 'F-score': 0.5923076923076923}, 'colour': {'Precision': 0.5714285714285714, 'Recall': 0.6666666666666666, 'F-score': 0.6153846153846153}, 'firmness': {'Precision': 0.16666666666666666, 'Recall': 0.5, 'F-score': 0.25}, 'mass': {'Precision': 0.92, 'Recall': 0.24468085106382978, 'F-score': 0.3865546218487395}, 'pathogen': {'Precision': 0.23529411764705882, 'Recall': 1.0, 'F-score': 0.38095238095238093}, 'size': {'Precision': 0.5277777777777778, 'Recall': 0.5277777777777778, 'F-score': 0.5277777777777778}, 'temperature': {'Precision': 0.8235294117647058, 'Recall': 1.0, 'F-score': 0.9032258064516129}, 'water content': {'Precision': 1.0, 'Recall': 0.5, 'F-score': 0.6666666666666666}, 'time constraint': {'Precision': 0.2, 'Recall': 0.058823529411764705, 'F-score': 0.0909090909090909}, 'overall': {'Precision': 0.49318801089918257, 'Recall': 0.861904761904762, 'F-score': 0.6273830155979202}})\n",
        "experiments_data.append({'data': {'Precision': 0.17403314917127072, 'Recall': 0.8873239436619719, 'F-score': 0.2909930715935335}, 'label data': {'Precision': 0.24607329842931938, 'Recall': 0.8703703703703703, 'F-score': 0.3836734693877551}, 'non-label data': {'Precision': 0.07878787878787878, 'Recall': 0.6842105263157895, 'F-score': 0.14130434782608695}, 'measurement': {'Precision': 0.6548672566371682, 'Recall': 0.5138888888888888, 'F-score': 0.5758754863813228}, 'colour': {'Precision': 0.6, 'Recall': 0.5, 'F-score': 0.5454545454545454}, 'firmness': {'Precision': 0.0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.8, 'Recall': 0.2127659574468085, 'F-score': 0.33613445378151263}, 'pathogen': {'Precision': 0.2222222222222222, 'Recall': 1.0, 'F-score': 0.3636363636363636}, 'size': {'Precision': 0.4473684210526316, 'Recall': 0.4722222222222222, 'F-score': 0.4594594594594595}, 'temperature': {'Precision': 0.8235294117647058, 'Recall': 1.0, 'F-score': 0.9032258064516129}, 'water content': {'Precision': 0.6666666666666666, 'Recall': 0.5, 'F-score': 0.5714285714285715}, 'time constraint': {'Precision': 0.3333333333333333, 'Recall': 0.11764705882352941, 'F-score': 0.1739130434782609}, 'overall': {'Precision': 0.49595687331536387, 'Recall': 0.8761904761904762, 'F-score': 0.6333907056798623}})\n",
        "experiments_data.append({'data': {'Precision': 0.18181818181818182, 'Recall': 0.9014084507042254, 'F-score': 0.3026004728132388}, 'label data': {'Precision': 0.2422680412371134, 'Recall': 0.8703703703703703, 'F-score': 0.3790322580645161}, 'non-label data': {'Precision': 0.10344827586206896, 'Recall': 0.7894736842105263, 'F-score': 0.18292682926829268}, 'measurement': {'Precision': 0.6213592233009708, 'Recall': 0.4444444444444444, 'F-score': 0.5182186234817814}, 'colour': {'Precision': 0.6666666666666666, 'Recall': 0.6666666666666666, 'F-score': 0.6666666666666666}, 'firmness': {'Precision': 0.0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.8636363636363636, 'Recall': 0.20212765957446807, 'F-score': 0.3275862068965517}, 'pathogen': {'Precision': 0.21052631578947367, 'Recall': 1.0, 'F-score': 0.34782608695652173}, 'size': {'Precision': 0.4, 'Recall': 0.3333333333333333, 'F-score': 0.3636363636363636}, 'temperature': {'Precision': 0.7777777777777778, 'Recall': 1.0, 'F-score': 0.8750000000000001}, 'water content': {'Precision': 0.6666666666666666, 'Recall': 0.5, 'F-score': 0.5714285714285715}, 'time constraint': {'Precision': 0.3333333333333333, 'Recall': 0.11764705882352941, 'F-score': 0.1739130434782609}, 'overall': {'Precision': 0.49725274725274726, 'Recall': 0.861904761904762, 'F-score': 0.6306620209059234}})\n",
        "experiments_data.append({'data': {'Precision': 0.17008797653958943, 'Recall': 0.8169014084507042, 'F-score': 0.2815533980582524}, 'label data': {'Precision': 0.22105263157894736, 'Recall': 0.7777777777777778, 'F-score': 0.3442622950819672}, 'non-label data': {'Precision': 0.09722222222222222, 'Recall': 0.7368421052631579, 'F-score': 0.17177914110429449}, 'measurement': {'Precision': 0.59375, 'Recall': 0.3958333333333333, 'F-score': 0.47500000000000003}, 'colour': {'Precision': 0.4666666666666667, 'Recall': 0.5833333333333334, 'F-score': 0.5185185185185186}, 'firmness': {'Precision': 0.0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.8235294117647058, 'Recall': 0.14893617021276595, 'F-score': 0.25225225225225223}, 'pathogen': {'Precision': 0.21052631578947367, 'Recall': 1.0, 'F-score': 0.34782608695652173}, 'size': {'Precision': 0.37037037037037035, 'Recall': 0.2777777777777778, 'F-score': 0.3174603174603175}, 'temperature': {'Precision': 0.7222222222222222, 'Recall': 0.9285714285714286, 'F-score': 0.8125000000000001}, 'water content': {'Precision': 1.0, 'Recall': 0.5, 'F-score': 0.6666666666666666}, 'time constraint': {'Precision': 0.4, 'Recall': 0.11764705882352941, 'F-score': 0.1818181818181818}, 'overall': {'Precision': 0.48295454545454547, 'Recall': 0.8095238095238095, 'F-score': 0.604982206405694}})\n",
        "experiments_data.append({'data': {'Precision': 0.1791907514450867, 'Recall': 0.8732394366197183, 'F-score': 0.2973621103117505}, 'label data': {'Precision': 0.225, 'Recall': 0.8333333333333334, 'F-score': 0.3543307086614173}, 'non-label data': {'Precision': 0.09701492537313433, 'Recall': 0.6842105263157895, 'F-score': 0.1699346405228758}, 'measurement': {'Precision': 0.6371681415929203, 'Recall': 0.5, 'F-score': 0.5603112840466925}, 'colour': {'Precision': 0.6153846153846154, 'Recall': 0.6666666666666666, 'F-score': 0.64}, 'firmness': {'Precision': 0.25, 'Recall': 0.5, 'F-score': 0.3333333333333333}, 'mass': {'Precision': 0.8823529411764706, 'Recall': 0.1595744680851064, 'F-score': 0.2702702702702703}, 'pathogen': {'Precision': 0.23529411764705882, 'Recall': 1.0, 'F-score': 0.38095238095238093}, 'size': {'Precision': 0.48717948717948717, 'Recall': 0.5277777777777778, 'F-score': 0.5066666666666667}, 'temperature': {'Precision': 0.875, 'Recall': 1.0, 'F-score': 0.9333333333333333}, 'water content': {'Precision': 1.0, 'Recall': 0.5, 'F-score': 0.6666666666666666}, 'time constraint': {'Precision': 0.25, 'Recall': 0.058823529411764705, 'F-score': 0.09523809523809523}, 'overall': {'Precision': 0.4905149051490515, 'Recall': 0.861904761904762, 'F-score': 0.6252158894645942}})\n",
        "experiments_data.append({'data': {'Precision': 0.18361581920903955, 'Recall': 0.9154929577464789, 'F-score': 0.3058823529411765}, 'label data': {'Precision': 0.24102564102564103, 'Recall': 0.8703703703703703, 'F-score': 0.3775100401606426}, 'non-label data': {'Precision': 0.09859154929577464, 'Recall': 0.7368421052631579, 'F-score': 0.17391304347826084}, 'measurement': {'Precision': 0.6330275229357798, 'Recall': 0.4791666666666667, 'F-score': 0.5454545454545454}, 'colour': {'Precision': 0.5833333333333334, 'Recall': 0.5833333333333334, 'F-score': 0.5833333333333334}, 'firmness': {'Precision': 0.0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.8333333333333334, 'Recall': 0.2127659574468085, 'F-score': 0.3389830508474576}, 'pathogen': {'Precision': 0.23529411764705882, 'Recall': 1.0, 'F-score': 0.38095238095238093}, 'size': {'Precision': 0.4864864864864865, 'Recall': 0.5, 'F-score': 0.4931506849315069}, 'temperature': {'Precision': 0.8235294117647058, 'Recall': 1.0, 'F-score': 0.9032258064516129}, 'water content': {'Precision': 0.6666666666666666, 'Recall': 0.5, 'F-score': 0.5714285714285715}, 'time constraint': {'Precision': 0.25, 'Recall': 0.058823529411764705, 'F-score': 0.09523809523809523}, 'overall': {'Precision': 0.49175824175824173, 'Recall': 0.8523809523809524, 'F-score': 0.6236933797909407}})\n",
        "experiments_data.append({'data': {'Precision': 0.1662269129287599, 'Recall': 0.8873239436619719, 'F-score': 0.28}, 'label data': {'Precision': 0.21904761904761905, 'Recall': 0.8518518518518519, 'F-score': 0.3484848484848485}, 'non-label data': {'Precision': 0.07975460122699386, 'Recall': 0.6842105263157895, 'F-score': 0.14285714285714288}, 'measurement': {'Precision': 0.5575221238938053, 'Recall': 0.4375, 'F-score': 0.49027237354085607}, 'colour': {'Precision': 0.5, 'Recall': 0.5833333333333334, 'F-score': 0.5384615384615384}, 'firmness': {'Precision': 0.0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.8333333333333334, 'Recall': 0.1595744680851064, 'F-score': 0.2678571428571429}, 'pathogen': {'Precision': 0.23529411764705882, 'Recall': 1.0, 'F-score': 0.38095238095238093}, 'size': {'Precision': 0.38095238095238093, 'Recall': 0.4444444444444444, 'F-score': 0.41025641025641024}, 'temperature': {'Precision': 0.8235294117647058, 'Recall': 1.0, 'F-score': 0.9032258064516129}, 'water content': {'Precision': 0.75, 'Recall': 0.75, 'F-score': 0.75}, 'time constraint': {'Precision': 0.2, 'Recall': 0.058823529411764705, 'F-score': 0.0909090909090909}, 'overall': {'Precision': 0.4740932642487047, 'Recall': 0.8714285714285714, 'F-score': 0.6140939597315437}})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJG-fq-IkHPt"
      },
      "source": [
        "Fine-tuning Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIbvycIcV4U5"
      },
      "outputs": [],
      "source": [
        "experiments_data = []\n",
        "experiments_data.append({'overall': {'Precision': 0.9067853457172342, 'Recall': 0.8500000000000001, 'F-score': 0.8771980676328504}, 'data': {'Precision': 0.7650243040212108, 'Recall': 0.7253521126760563, 'F-score': 0.743421052631579}, 'labeldata': {'Precision': 0.7763636363636364, 'Recall': 0.6759259259259259, 'F-score': 0.7219387755102042}, 'non-labeldata': {'Precision': 0.7427536231884058, 'Recall': 0.7894736842105263, 'F-score': 0.7625482625482626}, 'measurement': {'Precision': 0.9121212121212121, 'Recall': 0.8298611111111112, 'F-score': 0.8690362847773194}, 'colour': {'Precision': 0.9444444444444444, 'Recall': 0.6666666666666666, 'F-score': 0.7809523809523811}, 'firmness': {'Precision': 0.3333333333333333, 'Recall': 0.75, 'F-score': 0.45}, 'mass': {'Precision': 0.8822801674914351, 'Recall': 0.6808510638297872, 'F-score': 0.7685064935064935}, 'pathogen': {'Precision': 0.3125, 'Recall': 0.5, 'F-score': 0.375}, 'size': {'Precision': 0.8768939393939394, 'Recall': 0.7916666666666667, 'F-score': 0.8320545609548168}, 'temperature': {'Precision': 0.9615384615384616, 'Recall': 0.8571428571428571, 'F-score': 0.905982905982906}, 'water content': {'Precision': 0.5, 'Recall': 0.125, 'F-score': 0.2}, 'time constraint': {'Precision': 1.0, 'Recall': 0.2647058823529412, 'F-score': 0.41774891774891776}})\n",
        "experiments_data.append({'overall': {'Precision': 0.8998130179907013, 'Recall': 0.8523809523809525, 'F-score': 0.8753168795140384}, 'data': {'Precision': 0.7650243040212108, 'Recall': 0.7253521126760563, 'F-score': 0.743421052631579}, 'labeldata': {'Precision': 0.7763636363636364, 'Recall': 0.6759259259259259, 'F-score': 0.7219387755102042}, 'non-labeldata': {'Precision': 0.7427536231884058, 'Recall': 0.7894736842105263, 'F-score': 0.7625482625482626}, 'measurement': {'Precision': 0.9022058823529412, 'Recall': 0.8333333333333333, 'F-score': 0.8662930135557874}, 'colour': {'Precision': 0.8888888888888888, 'Recall': 0.6666666666666666, 'F-score': 0.761904761904762}, 'firmness': {'Precision': 0.3333333333333333, 'Recall': 0.75, 'F-score': 0.45}, 'mass': {'Precision': 0.876244131455399, 'Recall': 0.6808510638297872, 'F-score': 0.7661466738389815}, 'pathogen': {'Precision': 0.25, 'Recall': 0.375, 'F-score': 0.29166666666666663}, 'size': {'Precision': 0.8787878787878788, 'Recall': 0.8055555555555556, 'F-score': 0.8405797101449276}, 'temperature': {'Precision': 0.9615384615384616, 'Recall': 0.8571428571428571, 'F-score': 0.905982905982906}, 'water content': {'Precision': 0.5, 'Recall': 0.125, 'F-score': 0.2}, 'time constraint': {'Precision': 1.0, 'Recall': 0.2647058823529412, 'F-score': 0.41774891774891776}})\n",
        "experiments_data.append({'overall': {'Precision': 0.8998130179907013, 'Recall': 0.7571428571428571, 'F-score': 0.8302872062663186}, 'data': {'Precision': 0.9183673469387755, 'Recall': 0.6338028169014085, 'F-score': 0.7500000000000001}, 'labeldata': {'Precision': 0.9354838709677419, 'Recall': 0.5370370370370371, 'F-score': 0.6823529411764706}, 'non-labeldata': {'Precision': 0.8888888888888888, 'Recall': 0.8421052631578947, 'F-score': 0.8648648648648649}, 'measurement': {'Precision': 0.88, 'Recall': 0.7638888888888888, 'F-score': 0.8178438661710037}, 'colour': {'Precision': 0.8888888888888888, 'Recall': 0.6666666666666666, 'F-score': 0.761904761904762}, 'firmness': {'Precision': 0.5, 'Recall': 0.5, 'F-score': 0.5}, 'mass': {'Precision': 0.8793103448275862, 'Recall': 0.5425531914893617, 'F-score': 0.6710526315789473}, 'pathogen': {'Precision': 0.2222222222222222, 'Recall': 0.5, 'F-score': 0.30769230769230765}, 'size': {'Precision': 0.75, 'Recall': 0.75, 'F-score': 0.75}, 'temperature': {'Precision': 1.0, 'Recall': 0.8571428571428571, 'F-score': 0.923076923076923}, 'water content': {'Precision': 0, 'Recall': 0.0, 'F-score': 0}, 'time constraint': {'Precision': 1.0, 'Recall': 0.23529411764705882, 'F-score': 0.38095238095238093}})\n",
        "experiments_data.append({'overall': {'Precision': 0.8756756756756757, 'Recall': 0.7714285714285715, 'F-score': 0.8202531645569621}, 'data': {'Precision': 0.7678571428571429, 'Recall': 0.6056338028169014, 'F-score': 0.6771653543307087}, 'labeldata': {'Precision': 0.717948717948718, 'Recall': 0.5185185185185185, 'F-score': 0.6021505376344085}, 'non-labeldata': {'Precision': 0.8823529411764706, 'Recall': 0.7894736842105263, 'F-score': 0.8333333333333333}, 'measurement': {'Precision': 0.8721804511278195, 'Recall': 0.8055555555555556, 'F-score': 0.8375451263537906}, 'colour': {'Precision': 0.8888888888888888, 'Recall': 0.6666666666666666, 'F-score': 0.761904761904762}, 'firmness': {'Precision': 0.3333333333333333, 'Recall': 0.5, 'F-score': 0.4}, 'mass': {'Precision': 0.8695652173913043, 'Recall': 0.6382978723404256, 'F-score': 0.736196319018405}, 'pathogen': {'Precision': 0.2857142857142857, 'Recall': 0.5, 'F-score': 0.36363636363636365}, 'size': {'Precision': 0.8, 'Recall': 0.7777777777777778, 'F-score': 0.7887323943661971}, 'temperature': {'Precision': 1.0, 'Recall': 0.8571428571428571, 'F-score': 0.923076923076923}, 'water content': {'Precision': 0, 'Recall': 0.0, 'F-score': 0}, 'time constraint': {'Precision': 0.8, 'Recall': 0.23529411764705882, 'F-score': 0.3636363636363636}})\n",
        "experiments_data.append({'overall': {'Precision': 0.8804347826086957, 'Recall': 0.7714285714285715, 'F-score': 0.8223350253807107}, 'data': {'Precision': 0.7962962962962963, 'Recall': 0.6056338028169014, 'F-score': 0.688}, 'labeldata': {'Precision': 0.7567567567567568, 'Recall': 0.5185185185185185, 'F-score': 0.6153846153846154}, 'non-labeldata': {'Precision': 0.8823529411764706, 'Recall': 0.7894736842105263, 'F-score': 0.8333333333333333}, 'measurement': {'Precision': 0.8656716417910447, 'Recall': 0.8055555555555556, 'F-score': 0.8345323741007193}, 'colour': {'Precision': 0.8888888888888888, 'Recall': 0.6666666666666666, 'F-score': 0.761904761904762}, 'firmness': {'Precision': 0.3333333333333333, 'Recall': 0.5, 'F-score': 0.4}, 'mass': {'Precision': 0.8714285714285714, 'Recall': 0.648936170212766, 'F-score': 0.7439024390243902}, 'pathogen': {'Precision': 0.2857142857142857, 'Recall': 0.5, 'F-score': 0.36363636363636365}, 'size': {'Precision': 0.7777777777777778, 'Recall': 0.7777777777777778, 'F-score': 0.7777777777777778}, 'temperature': {'Precision': 1.0, 'Recall': 0.7857142857142857, 'F-score': 0.88}, 'water content': {'Precision': 0, 'Recall': 0.0, 'F-score': 0}, 'time constraint': {'Precision': 0.8, 'Recall': 0.23529411764705882, 'F-score': 0.3636363636363636}})\n",
        "experiments_data.append({'overall': {'Precision': 0.8810810810810811, 'Recall': 0.7761904761904762, 'F-score': 0.8253164556962025}, 'data': {'Precision': 0.8, 'Recall': 0.6197183098591549, 'F-score': 0.6984126984126985}, 'labeldata': {'Precision': 0.7631578947368421, 'Recall': 0.5370370370370371, 'F-score': 0.6304347826086957}, 'non-labeldata': {'Precision': 0.8823529411764706, 'Recall': 0.7894736842105263, 'F-score': 0.8333333333333333}, 'measurement': {'Precision': 0.8656716417910447, 'Recall': 0.8055555555555556, 'F-score': 0.8345323741007193}, 'colour': {'Precision': 0.8888888888888888, 'Recall': 0.6666666666666666, 'F-score': 0.761904761904762}, 'firmness': {'Precision': 0.3333333333333333, 'Recall': 0.5, 'F-score': 0.4}, 'mass': {'Precision': 0.8591549295774648, 'Recall': 0.648936170212766, 'F-score': 0.7393939393939394}, 'pathogen': {'Precision': 0.2857142857142857, 'Recall': 0.5, 'F-score': 0.36363636363636365}, 'size': {'Precision': 0.8, 'Recall': 0.7777777777777778, 'F-score': 0.7887323943661971}, 'temperature': {'Precision': 1.0, 'Recall': 0.7857142857142857, 'F-score': 0.88}, 'water content': {'Precision': 0, 'Recall': 0.0, 'F-score': 0}, 'time constraint': {'Precision': 0.8, 'Recall': 0.23529411764705882, 'F-score': 0.3636363636363636}})\n",
        "experiments_data.append({'overall': {'Precision': 0.8877551020408163, 'Recall': 0.8285714285714286, 'F-score': 0.8571428571428572}, 'data': {'Precision': 0.7540983606557377, 'Recall': 0.647887323943662, 'F-score': 0.6969696969696969}, 'labeldata': {'Precision': 0.775, 'Recall': 0.5740740740740741, 'F-score': 0.6595744680851063}, 'non-labeldata': {'Precision': 0.7142857142857143, 'Recall': 0.7894736842105263, 'F-score': 0.7500000000000001}, 'measurement': {'Precision': 0.9029850746268657, 'Recall': 0.8402777777777778, 'F-score': 0.8705035971223023}, 'colour': {'Precision': 0.875, 'Recall': 0.5833333333333334, 'F-score': 0.7000000000000001}, 'firmness': {'Precision': 0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.8947368421052632, 'Recall': 0.723404255319149, 'F-score': 0.8}, 'pathogen': {'Precision': 0.2, 'Recall': 0.25, 'F-score': 0.22222222222222224}, 'size': {'Precision': 0.8235294117647058, 'Recall': 0.7777777777777778, 'F-score': 0.7999999999999999}, 'temperature': {'Precision': 1.0, 'Recall': 0.8571428571428571, 'F-score': 0.923076923076923}, 'water content': {'Precision': 0, 'Recall': 0.0, 'F-score': 0}, 'time constraint': {'Precision': 0.75, 'Recall': 0.35294117647058826, 'F-score': 0.48}})\n",
        "experiments_data.append({'overall': {'Precision': 0.875, 'Recall': 0.8333333333333334, 'F-score': 0.8536585365853658}, 'data': {'Precision': 0.6666666666666666, 'Recall': 0.647887323943662, 'F-score': 0.6571428571428573}, 'labeldata': {'Precision': 0.6595744680851063, 'Recall': 0.5740740740740741, 'F-score': 0.613861386138614}, 'non-labeldata': {'Precision': 0.6818181818181818, 'Recall': 0.7894736842105263, 'F-score': 0.7317073170731707}, 'measurement': {'Precision': 0.9104477611940298, 'Recall': 0.8472222222222222, 'F-score': 0.8776978417266188}, 'colour': {'Precision': 0.875, 'Recall': 0.5833333333333334, 'F-score': 0.7000000000000001}, 'firmness': {'Precision': 0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.8947368421052632, 'Recall': 0.723404255319149, 'F-score': 0.8}, 'pathogen': {'Precision': 0.16666666666666666, 'Recall': 0.25, 'F-score': 0.2}, 'size': {'Precision': 0.8235294117647058, 'Recall': 0.7777777777777778, 'F-score': 0.7999999999999999}, 'temperature': {'Precision': 1.0, 'Recall': 0.8571428571428571, 'F-score': 0.923076923076923}, 'water content': {'Precision': 0, 'Recall': 0.0, 'F-score': 0}, 'time constraint': {'Precision': 0.75, 'Recall': 0.35294117647058826, 'F-score': 0.48}})\n",
        "experiments_data.append({'overall': {'Precision': 0.8923076923076924, 'Recall': 0.8285714285714286, 'F-score': 0.8592592592592593}, 'data': {'Precision': 0.71875, 'Recall': 0.647887323943662, 'F-score': 0.6814814814814815}, 'labeldata': {'Precision': 0.7209302325581395, 'Recall': 0.5740740740740741, 'F-score': 0.6391752577319588}, 'non-labeldata': {'Precision': 0.7142857142857143, 'Recall': 0.7894736842105263, 'F-score': 0.7500000000000001}, 'measurement': {'Precision': 0.9097744360902256, 'Recall': 0.8402777777777778, 'F-score': 0.8736462093862816}, 'colour': {'Precision': 0.875, 'Recall': 0.5833333333333334, 'F-score': 0.7000000000000001}, 'firmness': {'Precision': 0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.8947368421052632, 'Recall': 0.723404255319149, 'F-score': 0.8}, 'pathogen': {'Precision': 0.2, 'Recall': 0.25, 'F-score': 0.22222222222222224}, 'size': {'Precision': 0.8235294117647058, 'Recall': 0.7777777777777778, 'F-score': 0.7999999999999999}, 'temperature': {'Precision': 1.0, 'Recall': 0.8571428571428571, 'F-score': 0.923076923076923}, 'water content': {'Precision': 0, 'Recall': 0.0, 'F-score': 0}, 'time constraint': {'Precision': 0.6666666666666666, 'Recall': 0.35294117647058826, 'F-score': 0.46153846153846156}})\n",
        "experiments_data.append({'overall': {'Precision': 0.8872549019607843, 'Recall': 0.861904761904762, 'F-score': 0.8743961352657006}, 'data': {'Precision': 0.7361111111111112, 'Recall': 0.7464788732394366, 'F-score': 0.7412587412587414}, 'labeldata': {'Precision': 0.7755102040816326, 'Recall': 0.7037037037037037, 'F-score': 0.7378640776699029}, 'non-labeldata': {'Precision': 0.6521739130434783, 'Recall': 0.7894736842105263, 'F-score': 0.7142857142857143}, 'measurement': {'Precision': 0.9007633587786259, 'Recall': 0.8194444444444444, 'F-score': 0.858181818181818}, 'colour': {'Precision': 0.8888888888888888, 'Recall': 0.6666666666666666, 'F-score': 0.761904761904762}, 'firmness': {'Precision': 0.3333333333333333, 'Recall': 0.5, 'F-score': 0.4}, 'mass': {'Precision': 0.8714285714285714, 'Recall': 0.648936170212766, 'F-score': 0.7439024390243902}, 'pathogen': {'Precision': 0.25, 'Recall': 0.25, 'F-score': 0.25}, 'size': {'Precision': 0.8823529411764706, 'Recall': 0.8333333333333334, 'F-score': 0.8571428571428571}, 'temperature': {'Precision': 1.0, 'Recall': 0.9285714285714286, 'F-score': 0.962962962962963}, 'water content': {'Precision': 0, 'Recall': 0.0, 'F-score': 0}, 'time constraint': {'Precision': 1.0, 'Recall': 0.23529411764705882, 'F-score': 0.38095238095238093}})\n",
        "experiments_data.append({'overall': {'Precision': 0.8872549019607843, 'Recall': 0.861904761904762, 'F-score': 0.8743961352657006}, 'data': {'Precision': 0.7397260273972602, 'Recall': 0.7605633802816901, 'F-score': 0.75}, 'labeldata': {'Precision': 0.78, 'Recall': 0.7222222222222222, 'F-score': 0.7500000000000001}, 'non-labeldata': {'Precision': 0.6521739130434783, 'Recall': 0.7894736842105263, 'F-score': 0.7142857142857143}, 'measurement': {'Precision': 0.9, 'Recall': 0.8125, 'F-score': 0.854014598540146}, 'colour': {'Precision': 0.8888888888888888, 'Recall': 0.6666666666666666, 'F-score': 0.761904761904762}, 'firmness': {'Precision': 0.3333333333333333, 'Recall': 0.5, 'F-score': 0.4}, 'mass': {'Precision': 0.8591549295774648, 'Recall': 0.648936170212766, 'F-score': 0.7393939393939394}, 'pathogen': {'Precision': 0.25, 'Recall': 0.25, 'F-score': 0.25}, 'size': {'Precision': 0.8787878787878788, 'Recall': 0.8055555555555556, 'F-score': 0.8405797101449276}, 'temperature': {'Precision': 1.0, 'Recall': 0.8571428571428571, 'F-score': 0.923076923076923}, 'water content': {'Precision': 0, 'Recall': 0.0, 'F-score': 0}, 'time constraint': {'Precision': 1.0, 'Recall': 0.23529411764705882, 'F-score': 0.38095238095238093}})\n",
        "experiments_data.append({'overall': {'Precision': 0.882051282051282, 'Recall': 0.819047619047619, 'F-score': 0.8493827160493826}, 'data': {'Precision': 0.7361111111111112, 'Recall': 0.7464788732394366, 'F-score': 0.7412587412587414}, 'labeldata': {'Precision': 0.7169811320754716, 'Recall': 0.7037037037037037, 'F-score': 0.7102803738317758}, 'non-labeldata': {'Precision': 0.7894736842105263, 'Recall': 0.7894736842105263, 'F-score': 0.7894736842105263}, 'measurement': {'Precision': 0.8984375, 'Recall': 0.7986111111111112, 'F-score': 0.8455882352941176}, 'colour': {'Precision': 0.8, 'Recall': 0.3333333333333333, 'F-score': 0.47058823529411764}, 'firmness': {'Precision': 0.0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.88, 'Recall': 0.7021276595744681, 'F-score': 0.7810650887573964}, 'pathogen': {'Precision': 0.25, 'Recall': 0.25, 'F-score': 0.25}, 'size': {'Precision': 0.8484848484848485, 'Recall': 0.7777777777777778, 'F-score': 0.8115942028985507}, 'temperature': {'Precision': 1.0, 'Recall': 0.7857142857142857, 'F-score': 0.88}, 'water content': {'Precision': 0, 'Recall': 0.0, 'F-score': 0}, 'time constraint': {'Precision': 1.0, 'Recall': 0.23529411764705882, 'F-score': 0.38095238095238093}})\n",
        "experiments_data.append({'overall': {'Precision': 0.8793969849246231, 'Recall': 0.8333333333333334, 'F-score': 0.8557457212713937}, 'data': {'Precision': 0.7361111111111112, 'Recall': 0.7464788732394366, 'F-score': 0.7412587412587414}, 'labeldata': {'Precision': 0.7169811320754716, 'Recall': 0.7037037037037037, 'F-score': 0.7102803738317758}, 'non-labeldata': {'Precision': 0.7894736842105263, 'Recall': 0.7894736842105263, 'F-score': 0.7894736842105263}, 'measurement': {'Precision': 0.8939393939393939, 'Recall': 0.8194444444444444, 'F-score': 0.855072463768116}, 'colour': {'Precision': 0.8333333333333334, 'Recall': 0.4166666666666667, 'F-score': 0.5555555555555556}, 'firmness': {'Precision': 0.3333333333333333, 'Recall': 0.5, 'F-score': 0.4}, 'mass': {'Precision': 0.88, 'Recall': 0.7021276595744681, 'F-score': 0.7810650887573964}, 'pathogen': {'Precision': 0.25, 'Recall': 0.25, 'F-score': 0.25}, 'size': {'Precision': 0.8285714285714286, 'Recall': 0.8055555555555556, 'F-score': 0.8169014084507044}, 'temperature': {'Precision': 1.0, 'Recall': 0.7857142857142857, 'F-score': 0.88}, 'water content': {'Precision': 0, 'Recall': 0.0, 'F-score': 0}, 'time constraint': {'Precision': 1.0, 'Recall': 0.23529411764705882, 'F-score': 0.38095238095238093}})\n",
        "experiments_data.append({'overall': {'Precision': 0.9263157894736842, 'Recall': 0.8380952380952381, 'F-score': 0.88}, 'data': {'Precision': 0.7903225806451613, 'Recall': 0.6901408450704225, 'F-score': 0.7368421052631579}, 'labeldata': {'Precision': 0.7727272727272727, 'Recall': 0.6296296296296297, 'F-score': 0.6938775510204083}, 'non-labeldata': {'Precision': 0.8333333333333334, 'Recall': 0.7894736842105263, 'F-score': 0.8108108108108109}, 'measurement': {'Precision': 0.9242424242424242, 'Recall': 0.8472222222222222, 'F-score': 0.8840579710144927}, 'colour': {'Precision': 1.0, 'Recall': 0.6666666666666666, 'F-score': 0.8}, 'firmness': {'Precision': 0.3333333333333333, 'Recall': 1.0, 'F-score': 0.5}, 'mass': {'Precision': 0.9054054054054054, 'Recall': 0.7127659574468085, 'F-score': 0.7976190476190476}, 'pathogen': {'Precision': 0.375, 'Recall': 0.75, 'F-score': 0.5}, 'size': {'Precision': 0.875, 'Recall': 0.7777777777777778, 'F-score': 0.823529411764706}, 'temperature': {'Precision': 0.9230769230769231, 'Recall': 0.8571428571428571, 'F-score': 0.888888888888889}, 'water content': {'Precision': 1.0, 'Recall': 0.25, 'F-score': 0.4}, 'time constraint': {'Precision': 1.0, 'Recall': 0.29411764705882354, 'F-score': 0.45454545454545453}})\n",
        "experiments_data.append({'overall': {'Precision': 0.9123711340206185, 'Recall': 0.8428571428571429, 'F-score': 0.8762376237623761}, 'data': {'Precision': 0.7903225806451613, 'Recall': 0.6901408450704225, 'F-score': 0.7368421052631579}, 'labeldata': {'Precision': 0.7727272727272727, 'Recall': 0.6296296296296297, 'F-score': 0.6938775510204083}, 'non-labeldata': {'Precision': 0.8333333333333334, 'Recall': 0.7894736842105263, 'F-score': 0.8108108108108109}, 'measurement': {'Precision': 0.9044117647058824, 'Recall': 0.8541666666666666, 'F-score': 0.8785714285714286}, 'colour': {'Precision': 0.8888888888888888, 'Recall': 0.6666666666666666, 'F-score': 0.761904761904762}, 'firmness': {'Precision': 0.3333333333333333, 'Recall': 1.0, 'F-score': 0.5}, 'mass': {'Precision': 0.8933333333333333, 'Recall': 0.7127659574468085, 'F-score': 0.7928994082840236}, 'pathogen': {'Precision': 0.25, 'Recall': 0.5, 'F-score': 0.3333333333333333}, 'size': {'Precision': 0.8787878787878788, 'Recall': 0.8055555555555556, 'F-score': 0.8405797101449276}, 'temperature': {'Precision': 0.9230769230769231, 'Recall': 0.8571428571428571, 'F-score': 0.888888888888889}, 'water content': {'Precision': 1.0, 'Recall': 0.25, 'F-score': 0.4}, 'time constraint': {'Precision': 1.0, 'Recall': 0.29411764705882354, 'F-score': 0.45454545454545453}})\n",
        "experiments_data.append({'overall': {'Precision': 0.90625, 'Recall': 0.8285714285714286, 'F-score': 0.8656716417910447}, 'data': {'Precision': 0.8103448275862069, 'Recall': 0.6619718309859155, 'F-score': 0.7286821705426356}, 'labeldata': {'Precision': 0.8648648648648649, 'Recall': 0.5925925925925926, 'F-score': 0.7032967032967032}, 'non-labeldata': {'Precision': 0.7142857142857143, 'Recall': 0.7894736842105263, 'F-score': 0.7500000000000001}, 'measurement': {'Precision': 0.8947368421052632, 'Recall': 0.8263888888888888, 'F-score': 0.8592057761732851}, 'colour': {'Precision': 0.8888888888888888, 'Recall': 0.6666666666666666, 'F-score': 0.761904761904762}, 'firmness': {'Precision': 0.0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.9014084507042254, 'Recall': 0.6808510638297872, 'F-score': 0.7757575757575758}, 'pathogen': {'Precision': 0.3076923076923077, 'Recall': 1.0, 'F-score': 0.47058823529411764}, 'size': {'Precision': 0.8484848484848485, 'Recall': 0.7777777777777778, 'F-score': 0.8115942028985507}, 'temperature': {'Precision': 1.0, 'Recall': 0.7857142857142857, 'F-score': 0.88}, 'water content': {'Precision': 1.0, 'Recall': 0.25, 'F-score': 0.4}, 'time constraint': {'Precision': 1.0, 'Recall': 0.29411764705882354, 'F-score': 0.45454545454545453}})\n",
        "experiments_data.append({'overall': {'Precision': 0.9047619047619048, 'Recall': 0.8142857142857143, 'F-score': 0.857142857142857}, 'data': {'Precision': 0.8518518518518519, 'Recall': 0.647887323943662, 'F-score': 0.7360000000000001}, 'labeldata': {'Precision': 0.9117647058823529, 'Recall': 0.5740740740740741, 'F-score': 0.7045454545454545}, 'non-labeldata': {'Precision': 0.75, 'Recall': 0.7894736842105263, 'F-score': 0.7692307692307692}, 'measurement': {'Precision': 0.8731343283582089, 'Recall': 0.8125, 'F-score': 0.841726618705036}, 'colour': {'Precision': 0.8888888888888888, 'Recall': 0.6666666666666666, 'F-score': 0.761904761904762}, 'firmness': {'Precision': 0.0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.863013698630137, 'Recall': 0.6702127659574468, 'F-score': 0.7544910179640719}, 'pathogen': {'Precision': 0.3333333333333333, 'Recall': 1.0, 'F-score': 0.5}, 'size': {'Precision': 0.8529411764705882, 'Recall': 0.8055555555555556, 'F-score': 0.8285714285714286}, 'temperature': {'Precision': 1.0, 'Recall': 0.7857142857142857, 'F-score': 0.88}, 'water content': {'Precision': 1.0, 'Recall': 0.25, 'F-score': 0.4}, 'time constraint': {'Precision': 1.0, 'Recall': 0.29411764705882354, 'F-score': 0.45454545454545453}})\n",
        "experiments_data.append({'overall': {'Precision': 0.863849765258216, 'Recall': 0.8761904761904762, 'F-score': 0.8699763593380615}, 'data': {'Precision': 0.7571428571428571, 'Recall': 0.7464788732394366, 'F-score': 0.75177304964539}, 'labeldata': {'Precision': 0.8085106382978723, 'Recall': 0.7037037037037037, 'F-score': 0.7524752475247524}, 'non-labeldata': {'Precision': 0.6521739130434783, 'Recall': 0.7894736842105263, 'F-score': 0.7142857142857143}, 'measurement': {'Precision': 0.8278145695364238, 'Recall': 0.8680555555555556, 'F-score': 0.847457627118644}, 'colour': {'Precision': 0.6666666666666666, 'Recall': 0.6666666666666666, 'F-score': 0.6666666666666666}, 'firmness': {'Precision': 0.5, 'Recall': 0.5, 'F-score': 0.5}, 'mass': {'Precision': 0.8333333333333334, 'Recall': 0.7446808510638298, 'F-score': 0.7865168539325842}, 'pathogen': {'Precision': 0.14285714285714285, 'Recall': 0.25, 'F-score': 0.18181818181818182}, 'size': {'Precision': 0.8, 'Recall': 0.8888888888888888, 'F-score': 0.8421052631578948}, 'temperature': {'Precision': 0.9166666666666666, 'Recall': 0.7857142857142857, 'F-score': 0.8461538461538461}, 'water content': {'Precision': 0.0, 'Recall': 0.0, 'F-score': 0}, 'time constraint': {'Precision': 0.8, 'Recall': 0.23529411764705882, 'F-score': 0.3636363636363636}})\n",
        "experiments_data.append({'overall': {'Precision': 0.8591549295774648, 'Recall': 0.8714285714285714, 'F-score': 0.8652482269503546}, 'data': {'Precision': 0.828125, 'Recall': 0.7464788732394366, 'F-score': 0.7851851851851853}, 'labeldata': {'Precision': 0.926829268292683, 'Recall': 0.7037037037037037, 'F-score': 0.8000000000000002}, 'non-labeldata': {'Precision': 0.6521739130434783, 'Recall': 0.7894736842105263, 'F-score': 0.7142857142857143}, 'measurement': {'Precision': 0.8266666666666667, 'Recall': 0.8611111111111112, 'F-score': 0.8435374149659863}, 'colour': {'Precision': 0.7272727272727273, 'Recall': 0.6666666666666666, 'F-score': 0.6956521739130435}, 'firmness': {'Precision': 0.3333333333333333, 'Recall': 0.5, 'F-score': 0.4}, 'mass': {'Precision': 0.8352941176470589, 'Recall': 0.7553191489361702, 'F-score': 0.7932960893854748}, 'pathogen': {'Precision': 0.125, 'Recall': 0.25, 'F-score': 0.16666666666666666}, 'size': {'Precision': 0.7948717948717948, 'Recall': 0.8611111111111112, 'F-score': 0.8266666666666667}, 'temperature': {'Precision': 0.8461538461538461, 'Recall': 0.7857142857142857, 'F-score': 0.8148148148148148}, 'water content': {'Precision': 0.0, 'Recall': 0.0, 'F-score': 0}, 'time constraint': {'Precision': 0.8, 'Recall': 0.23529411764705882, 'F-score': 0.3636363636363636}})\n",
        "experiments_data.append({'overall': {'Precision': 0.8984, 'Recall':0.8 , 'F-score': 0.8463}, 'data': {'Precision': 0.8771929824561403, 'Recall': 0.704225352112676, 'F-score': 0.7812499999999999}, 'labeldata': {'Precision': 0.9444444444444444, 'Recall': 0.6296296296296297, 'F-score': 0.7555555555555556}, 'non-labeldata': {'Precision': 0.8, 'Recall': 0.8421052631578947, 'F-score': 0.8205128205128205}, 'measurement': {'Precision': 0.875, 'Recall': 0.7777777777777778, 'F-score': 0.823529411764706}, 'colour': {'Precision': 0.9, 'Recall': 0.75, 'F-score': 0.8181818181818182}, 'firmness': {'Precision': 0.0, 'Recall': 0.0, 'F-score': 0}, 'mass': {'Precision': 0.90625, 'Recall': 0.6170212765957447, 'F-score': 0.7341772151898734}, 'pathogen': {'Precision': 0.5, 'Recall': 0.5, 'F-score': 0.5}, 'size': {'Precision': 0.8787878787878788, 'Recall': 0.8055555555555556, 'F-score': 0.8405797101449276}, 'temperature': {'Precision': 1.0, 'Recall': 0.7857142857142857, 'F-score': 0.88}, 'water content': {'Precision': 1.0, 'Recall': 0.25, 'F-score': 0.4}, 'time constraint': {'Precision': 1.0,'Recall': 0.2941,'F-score': 0.4545}})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woJ-gUNeztvt"
      },
      "outputs": [],
      "source": [
        "label_names = ['data', 'labeldata', 'non-labeldata', 'measurement', 'colour', 'firmness', 'mass', 'pathogen', 'size', 'temperature', 'water content', 'time constraint','overall']\n",
        "accumulated_metrics = {label: {'Precision': 0, 'Recall': 0, 'F-score': 0} for label in label_names}\n",
        "averages = {label: {'Precision': 0, 'Recall': 0, 'F-score': 0} for label in label_names}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUx_8LWvzYd6"
      },
      "outputs": [],
      "source": [
        "for experiment in experiments_data:\n",
        "    for label, metrics in experiment.items():\n",
        "        accumulated_metrics[label]['Precision'] += metrics['Precision']\n",
        "        accumulated_metrics[label]['Recall'] += metrics['Recall']\n",
        "        accumulated_metrics[label]['F-score'] += metrics['F-score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztS7fMY_ybKV"
      },
      "outputs": [],
      "source": [
        "for label, metrics in accumulated_metrics.items():\n",
        "    metrics['Precision'] /= 20\n",
        "    metrics['Recall'] /= 20\n",
        "    metrics['F-score'] /= 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lmUHaDqybMX"
      },
      "outputs": [],
      "source": [
        "for label, metrics in accumulated_metrics.items():\n",
        "    print(f\"Label: {label}\")\n",
        "    print(f\"Average Precision: {metrics['Precision']}\")\n",
        "    print(f\"Average Recall: {metrics['Recall']}\")\n",
        "    print(f\"Average F-score: {metrics['F-score']}\")\n",
        "    print(\"------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ig4qU6ijywVY"
      },
      "outputs": [],
      "source": [
        "# Create empty lists for each metric of each label\n",
        "accumulated_metrics = {label: {'Precision': [], 'Recall': [], 'F-score': []} for label in experiments_data[0].keys()}\n",
        "\n",
        "# Accumulate the metrics for all experiments\n",
        "for experiment in experiments_data:\n",
        "    for label, metrics in experiment.items():\n",
        "        accumulated_metrics[label]['Precision'].append(metrics['Precision'])\n",
        "        accumulated_metrics[label]['Recall'].append(metrics['Recall'])\n",
        "        accumulated_metrics[label]['F-score'].append(metrics['F-score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtLUc9iG5vLO"
      },
      "outputs": [],
      "source": [
        "for label, metrics in accumulated_metrics.items():\n",
        "    print(metrics['Precision'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32-1oO6peBFn"
      },
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPiUiM_RB3tI"
      },
      "outputs": [],
      "source": [
        "def visualization(metrics_data, metric_name):\n",
        "    \"\"\"\n",
        "    Visualize a given metric.\n",
        "\n",
        "    Args:\n",
        "    - metrics_data (dict): A dictionary containing lists of metric values for each label.\n",
        "    - metric_name (str): Name of the metric being visualized (e.g., \"Precision\", \"Recall\").\n",
        "    \"\"\"\n",
        "\n",
        "    ticks = ['Overall', 'Measurement', 'Time\\nConstraint', 'Data',\n",
        "             'Temperature', 'Mass', 'Size',\n",
        "             'Non-label\\nData', 'Label\\nData']\n",
        "\n",
        "    formatted_ticks = {\n",
        "    'Time\\nConstraint': 'time constraint',\n",
        "    'Non-label\\nData': 'non-labeldata',\n",
        "    'Label\\nData': 'labeldata'\n",
        "    }\n",
        "\n",
        "    data = [metrics_data[formatted_ticks[tick] if tick in formatted_ticks else tick.lower().replace('\\n', '')] for tick in ticks]\n",
        "\n",
        "    #data = [metrics_data[tick.lower().replace('\\n', '')] for tick in ticks]\n",
        "\n",
        "    means = [Average(item) for item in data]\n",
        "\n",
        "    positions = np.array(np.arange(0, 12, 1))\n",
        "    widths = 0.4\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7, 7))\n",
        "\n",
        "    def define_box_properties(plot_name, color_code):\n",
        "        for k, v in plot_name.items():\n",
        "            plt.setp(plot_name.get(k), color=color_code, linewidth=2)\n",
        "\n",
        "        # Use plot function to draw a small line to name the legend.\n",
        "        plt.plot([], c=color_code)\n",
        "\n",
        "    for i, d in enumerate(data):\n",
        "        bp = ax.boxplot(d, positions=[i], widths=widths, showmeans=True, vert=False, sym='r+')\n",
        "        define_box_properties(bp, 'blue')\n",
        "\n",
        "        for line in bp['means']:\n",
        "            x, y = line.get_xydata()[0][0], line.get_xydata()[0][1]\n",
        "            text = '{:.2f}'.format(means[i])\n",
        "            plt.annotate(text, xy=(x - 0.01, y + 0.21), fontsize=15, fontweight='bold')\n",
        "\n",
        "    # Set the title and other plot details\n",
        "    plt.title(f\"GPT {metric_name}\", fontsize=15, fontweight='bold')\n",
        "    plt.yticks(np.array(np.arange(0, 9)), ticks, fontsize=15, fontweight='bold')\n",
        "    plt.xticks(fontsize=15, fontweight='bold')\n",
        "    plt.xticks(np.arange(0, 1.1, 0.2))\n",
        "    plt.savefig(f'GPT {metric_name}.pdf', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Average function (assuming you have it defined elsewhere in your code)\n",
        "def Average(lst):\n",
        "    return sum(lst) / len(lst)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DDMv_rK3C39"
      },
      "outputs": [],
      "source": [
        "def extract_metric_data(accumulated_metrics, metric_name):\n",
        "    return {label: metrics[metric_name] for label, metrics in accumulated_metrics.items()}\n",
        "\n",
        "accumulated_metrics_for_precision = extract_metric_data(accumulated_metrics, 'Precision')\n",
        "accumulated_metrics_for_recall = extract_metric_data(accumulated_metrics, 'Recall')\n",
        "accumulated_metrics_for_fscore = extract_metric_data(accumulated_metrics, 'F-score')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTCZFU-IybVb"
      },
      "outputs": [],
      "source": [
        "visualization(accumulated_metrics_for_precision, 'Precision')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ8FCzmQ4Tx4"
      },
      "outputs": [],
      "source": [
        "visualization(accumulated_metrics_for_recall, 'Recall')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUhgbsLK4c_E"
      },
      "outputs": [],
      "source": [
        "visualization(accumulated_metrics_for_fscore, 'F-score')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dItbbFaMeEO0"
      },
      "source": [
        "Statistical significance tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlQ7DYLZd27F"
      },
      "outputs": [],
      "source": [
        "# Assuming you named your csv file as \"bert_data.csv\"\n",
        "bert_df = pd.read_csv(\"Evaluation Results/RQ1/dfboxplots/df_boxplotBERTbase.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFPdXpGsej_D"
      },
      "outputs": [],
      "source": [
        "bert_df[0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oE5Haw8pYPr"
      },
      "outputs": [],
      "source": [
        "bert_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iO_jWrXM-R0"
      },
      "outputs": [],
      "source": [
        "#wilcoxcon significance test\n",
        "def wilcoxcon(lst1,lst2):\n",
        "\n",
        "  # perform the Wilcoxon rank-sum test\n",
        "  statistic, p_value = ranksums(lst1, lst2)\n",
        "  # print the test results\n",
        "  # print(\"Wilcoxon rank-sum test:\")\n",
        "  return(p_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G84ieZkjM-cR"
      },
      "outputs": [],
      "source": [
        "#asymetric vargha delany significance test\n",
        "def Average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "def a12(lst1,lst2,rev=True):\n",
        "      if Average(lst1) < Average(lst2):\n",
        "        rev=False\n",
        "\n",
        "      more = same = 0.0\n",
        "      for x in lst1:\n",
        "          for y in lst2:\n",
        "              if   x==y : same += 1\n",
        "              elif rev     and x > y : more += 1\n",
        "              elif not rev and x < y : more += 1\n",
        "      res = (more + 0.5*same)  / (len(lst1)*len(lst2))\n",
        "      if   0.71 <res :\n",
        "        description = 'Large'\n",
        "      elif 0.64 <res <=0.71:\n",
        "        description = 'Medium'\n",
        "      elif 0.56 <res <= 0.64:\n",
        "        description = 'Small'\n",
        "      elif res <= 0.56:\n",
        "        description = 'negligible'\n",
        "\n",
        "      if rev==False:\n",
        "        res=1-res\n",
        "        if res<0.29:\n",
        "          description = 'Large'\n",
        "        elif 0.29<=res<0.36:\n",
        "          description = 'Medium'\n",
        "        elif 0.36<=res<0.44:\n",
        "          description = 'Small'\n",
        "        elif 0.44<=res:\n",
        "          description = 'negligible'\n",
        "      return res, description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbPF9HizgfCM"
      },
      "outputs": [],
      "source": [
        "mapping = {\n",
        "    'Measurement': 'measurement',\n",
        "    'Temperature': 'temperature',\n",
        "    'Mass': 'mass',\n",
        "    'Size': 'size',\n",
        "    'Time Constraint': 'time constraint',\n",
        "    'Non-labelData': 'non-labeldata',\n",
        "    'LabelData': 'labeldata',\n",
        "    'Data': 'data',\n",
        "    'Overall': 'overall',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK3tVmXgfDXX"
      },
      "outputs": [],
      "source": [
        "# Create an empty dataframe with labels as columns\n",
        "df_statistic = pd.DataFrame(columns=['Label', 'Metric', 'A12', 'P-value'])\n",
        "\n",
        "for bert_label, gpt_label in mapping.items():\n",
        "    # Step 1: Extract BERT data\n",
        "    precision_bert_data = bert_df[bert_label][:20].tolist()\n",
        "    recall_bert_data = bert_df[bert_label][20:40].tolist()\n",
        "    fscore_bert_data = bert_df[bert_label][40:60].tolist()\n",
        "\n",
        "    # Step 2: Extract GPT data (using gpt_label from the mapping)\n",
        "    precision_gpt_data = [experiment[gpt_label]['Precision'] for experiment in experiments_data]\n",
        "    recall_gpt_data = [experiment[gpt_label]['Recall'] for experiment in experiments_data]\n",
        "    fscore_gpt_data = [experiment[gpt_label]['F-score'] for experiment in experiments_data]\n",
        "\n",
        "\n",
        "    # Step 3: Compute the A12 statistic\n",
        "    precision_a12_result = a12(precision_bert_data, precision_gpt_data)\n",
        "    recall_a12_result = a12(recall_bert_data, recall_gpt_data)\n",
        "    fscore_a12_result = a12(fscore_bert_data, fscore_gpt_data)\n",
        "\n",
        "    # Step 4: Compute the Wilcoxon rank-sum test p-values\n",
        "    precision_p_value = wilcoxcon(precision_bert_data, precision_gpt_data)\n",
        "    recall_p_value = wilcoxcon(recall_bert_data, recall_gpt_data)\n",
        "    fscore_p_value = wilcoxcon(fscore_bert_data, fscore_gpt_data)\n",
        "\n",
        "    # Append to dataframe\n",
        "    df_statistic = df_statistic.append({\n",
        "        'Label': bert_label,\n",
        "        'Metric': 'Precision',\n",
        "        'A12': precision_a12_result,\n",
        "        'P-value': precision_p_value\n",
        "    }, ignore_index=True)\n",
        "\n",
        "    df_statistic = df_statistic.append({\n",
        "        'Label': bert_label,\n",
        "        'Metric': 'Recall',\n",
        "        'A12': recall_a12_result,\n",
        "        'P-value': recall_p_value\n",
        "    }, ignore_index=True)\n",
        "\n",
        "    df_statistic = df_statistic.append({\n",
        "        'Label': bert_label,\n",
        "        'Metric': 'F-score',\n",
        "        'A12': fscore_a12_result,\n",
        "        'P-value': fscore_p_value\n",
        "    }, ignore_index=True)\n",
        "\n",
        "    print(f\"Label: {gpt_label}\")\n",
        "    print(f\"Precision A12 result: {precision_a12_result} | P-value: {precision_p_value}\")\n",
        "    print(f\"Recall A12 result: {recall_a12_result} | P-value: {recall_p_value}\")\n",
        "    print(f\"F-score A12 result: {fscore_a12_result} | P-value: {fscore_p_value}\")\n",
        "    print(\"============================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqbhxVfnNFBe"
      },
      "outputs": [],
      "source": [
        "df_statistic.to_csv(\"statBERTbase-GPT.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MHtSY_19GNH"
      },
      "source": [
        "Finetuning GPT3.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEJG2-vSDop_"
      },
      "source": [
        "\n",
        "\n",
        "1.   prepare data\n",
        "2.   upload data\n",
        "3.   create fine tune job\n",
        "4.   use fine-tuned model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3ag4HrqNFEM"
      },
      "outputs": [],
      "source": [
        "# Load the Excel file\n",
        "xlsx_file = pd.ExcelFile('Data/SFCR_1.xlsx')\n",
        "\n",
        "# Get the sheet names\n",
        "sheet_names = xlsx_file.sheet_names\n",
        "\n",
        "# Create dataframes with specific names\n",
        "df_sentences = xlsx_file.parse(sheet_names[0])  # Assuming the first sheet contains sentences\n",
        "df_paragraphs = xlsx_file.parse(sheet_names[1])  # Assuming the second sheet contains paragraphs\n",
        "\n",
        "# Removing rows where 'paragraph_id' is NaN\n",
        "df_paragraphs = df_paragraphs.dropna(subset=['paragraph_id'])\n",
        "# Resetting the index (optional)\n",
        "df_paragraphs = df_paragraphs.reset_index(drop=True)\n",
        "# Replace NaN values with 0 in df_sentences\n",
        "df_sentences.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp4sylCVMe3A"
      },
      "source": [
        "Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVSZqVP03a6l",
        "outputId": "2d18be66-15bd-4c66-b3b2-b764ba7db83b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSON data generated and saved to test_data.json\n"
          ]
        }
      ],
      "source": [
        "delimiter=\"%%\"\n",
        "# Specified columns\n",
        "specified_columns = ['Overall', 'Data', 'LabelData', 'Non-labelData', 'Measurement', 'Temperature', 'Size', 'Mass', 'Water Content', 'Pathogen', 'Firmness', 'Colour', 'Time Constraint']\n",
        "\n",
        "# Convert paragraphs dataframe to dictionary\n",
        "paragraphs = {int(row['paragraph_id']): row['Statement'] for _, row in df_paragraphs.iterrows()}\n",
        "\n",
        "# Convert sentences dataframe to dictionary\n",
        "sentences = {}\n",
        "for _, row in df_sentences.iterrows():\n",
        "    pid = int(row['paragraph_id'])\n",
        "    if pid not in sentences:\n",
        "        sentences[pid] = []\n",
        "\n",
        "    # Extract specified labels for each sentence\n",
        "    labels = [str(int(float(row[col]))) for col in specified_columns]\n",
        "    sentences[pid].append((row['Statement'], labels))\n",
        "\n",
        "\n",
        "\n",
        "# Generate JSON data\n",
        "test_data = []\n",
        "for pid, section in paragraphs.items():\n",
        "    user_message = section\n",
        "    assistant_message = ' #### '.join([f\"{sentence} ::: {', '.join(labels)}\" for sentence, labels in sentences[pid]])\n",
        "    test_data.append({\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"\"\"You are an asssitsant tasked with extracting relevant text segments and their labels from the provided food safety paragraph. The entire paragraph is delimited within {delimiter} characters. Use only the provided labels in this exact order: 'Overall', 'Data', 'Label Data', 'Non-label Data', 'Measurement', 'Temperature', 'Size', 'Mass', 'Water Content', 'Pathogen', 'Firmness', 'Colour', 'Time Constraint'.\n",
        "-Data: any information used to convey knowledge, provide assurance, or perform analysis. This includes 'Label Data' and 'Non-label Data'.-Label Data: a subtype of 'Data' that includes information that a food-product package or container must bear.-Non-label Data: a subtype of 'Data' that includes any food-safety-relevant data other than \\\n",
        "label data that needs to be collected and/or retained for inclusion in documents such as certificates, reports, guarantees, and letters.-Measurement: Association of numbers with physical quantities. This includes measurements of 'Colour', 'Firmness', 'Mass', 'Pathogen', 'Size', 'Temperature', and 'Water Content'.-Colour: a subtype of 'Measurement' \\\n",
        "that is self-evident.-Firmness: a subtype of 'Measurement' that refers to the degree of resistance to deformation.-Mass: a subtype of 'Measurement' that refers to the amount of substance by weight or volume.-Pathogen: a subtype of 'Measurement' that refers to a microorganism that causes disease.-Size: a subtype of 'Measurement' that refers to dimension \\\n",
        "(e.g., length or thickness) or surface area.-Temperature: a subtype of 'Measurement' that is self-evident.-Water Content: a subtype of 'Measurement' that refers to humidity or moisture.- Time Constraint: A temporal restriction, in our context, is expressed using intervals, deadlines or periodicity.-Overall: requirements-related provisions that include all the introduced concepts.\"\"\"\n",
        "\n",
        "\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"{delimiter}{user_message}{delimiter}\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": assistant_message\n",
        "            }\n",
        "        ]\n",
        "    })\n",
        "\n",
        "# Save to JSON file\n",
        "with open('test_data.json', 'w') as file:\n",
        "    json.dump(test_data, file, indent=4)\n",
        "\n",
        "print(\"JSON data generated and saved to test_data.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbW-C9pt40iT"
      },
      "outputs": [],
      "source": [
        "# Load the JSON data\n",
        "with open('fine_tuning_data.json', 'r') as file:\n",
        "    data = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acunlUFHnXCu"
      },
      "source": [
        "Number of examples for fine-tuning data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPN5AbGBL7D6"
      },
      "outputs": [],
      "source": [
        "# Initial dataset stats\n",
        "print(\"Num examples:\", len(data))\n",
        "print(\"First example:\")\n",
        "for message in data[0][\"messages\"]:\n",
        "    print(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkNXL4-eK112"
      },
      "outputs": [],
      "source": [
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in data:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        if not content or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGHSkQSBMmNp"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlxoCDamMJQm"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "# Token counting functions\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# not exact!\n",
        "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1M6TUJPMJS1"
      },
      "outputs": [],
      "source": [
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "for ex in data:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KpkLLdM1uHG"
      },
      "source": [
        "Total number of tokens for fine-tuning data with 3 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G2r5E8QMJX3"
      },
      "outputs": [],
      "source": [
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "MIN_DEFAULT_EPOCHS = 1\n",
        "MAX_DEFAULT_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(data)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
        "print(\"See pricing page to estimate total costs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjhxqpmFmrSv"
      },
      "source": [
        "Total number of tokens for fine-tuning data with 2 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWLxLTk8N8TB"
      },
      "outputs": [],
      "source": [
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "TARGET_EPOCHS = 2\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "MIN_DEFAULT_EPOCHS = 1\n",
        "MAX_DEFAULT_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(data)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
        "print(\"See pricing page to estimate total costs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJXYmpOVb90z"
      },
      "source": [
        "Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CN7ewg3LV9y"
      },
      "outputs": [],
      "source": [
        "def save_to_jsonl(conversations, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for conversation in conversations:\n",
        "            json_line = json.dumps(conversation)\n",
        "            file.write(json_line + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0Xu-5Azo2O7"
      },
      "outputs": [],
      "source": [
        "# Load the JSON data\n",
        "with open('fine_tuning_data.json', 'r') as file:\n",
        "    sampledata = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs0o8UK_LtdN"
      },
      "outputs": [],
      "source": [
        "save_to_jsonl(sampledata,'fine_tuning.jsonl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPgRuaHJXFy6"
      },
      "source": [
        "Upload Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-SOl59kMXbA"
      },
      "outputs": [],
      "source": [
        "training_file_name = 'fine_tuning.jsonl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLSNkqeiYVI-"
      },
      "outputs": [],
      "source": [
        "with open(\"fine_tuning.jsonl\") as file:\n",
        "  response=openai.File.create(\n",
        "      file=file,\n",
        "      purpose='fine-tune'\n",
        "  )\n",
        "file_id=response['id']\n",
        "print(f\"file uploaded successfully with ID:{file_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiE67zR2b23o"
      },
      "source": [
        "Create fine tune job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUcObONXMtsk"
      },
      "outputs": [],
      "source": [
        "suffix_name = \"RE2024-test\"\n",
        "\n",
        "\n",
        "response = openai.FineTuningJob.create(\n",
        "    training_file=\"file-SRaoYklhSeW95KmGXEz8FnGS\",\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    suffix=suffix_name,\n",
        "    hyperparameters={\"n_epochs\":3}\n",
        ")\n",
        "\n",
        "job_id = response[\"id\"]\n",
        "print(f\"Fine-tuning job created successfully with ID:{job_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-mWrGJnMxHm"
      },
      "outputs": [],
      "source": [
        "# response = openai.FineTuningJob.retrieve(job_id)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCwAgRcfMxKK"
      },
      "outputs": [],
      "source": [
        "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
        "\n",
        "events = response[\"data\"]\n",
        "events.reverse()\n",
        "\n",
        "for event in events:\n",
        "    print(event[\"message\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yst4pNTAbkPw"
      },
      "source": [
        "Use fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZrW72nJoymG"
      },
      "outputs": [],
      "source": [
        "# Load the Excel file\n",
        "df1 = pd.ExcelFile('Data/Annotation_1.xlsx')\n",
        "\n",
        "# Get the sheet names\n",
        "sheet_names = df1.sheet_names\n",
        "\n",
        "# Create dataframes with specific names\n",
        "df_sentences = df1.parse(sheet_names[0])  # Assuming the first sheet contains sentences\n",
        "df_paragraphs = df1.parse(sheet_names[1])  # Assuming the second sheet contains paragraphs\n",
        "# Replace NaN values with 0 in df_sentences\n",
        "df_sentences.fillna(0, inplace=True)\n",
        "\n",
        "# Replace NaN values with 0 in df_paragraphs\n",
        "df_paragraphs.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzMM2EcnoArw"
      },
      "outputs": [],
      "source": [
        "def get_completion_from_messages(messages,\n",
        "                                 model=\"ft:gpt-3.5-turbo-0613:university-of-ottawa:re2024-test:8CbY4mSJ\",\n",
        "                                 temperature=0.4,\n",
        "                                 max_tokens=None):\n",
        "    # If max_tokens is not provided, calculate it based on the message tokens\n",
        "    if max_tokens is None:\n",
        "        input_tokens = num_tokens_from_messages(messages)\n",
        "        max_tokens = 4096 - input_tokens\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKiLkRC5_1Ax"
      },
      "outputs": [],
      "source": [
        "system_message=f\"\"\"Your task is to extract sentences from the provided food safety paragraph and label them based on the concepts they contain. The paragraph is enclosed within %% characters. Use only the following labels: 'Overall', 'Data', 'Label Data', 'Non-label Data', 'Measurement', 'Temperature', 'Size', 'Mass', 'Water Content', 'Pathogen', 'Firmness', 'Colour', 'Time Constraint'. If a concept is present in a given sentence, label the sentence accordingly. The concepts are defined as follows: -Data: any information used to convey knowledge, provide assurance, or perform analysis. This includes 'Label Data' and 'Non-label Data'. -Label Data: a subtype of 'Data' that includes information that a food-product package or container must bear. -Non-label Data: a subtype of 'Data' that includes any food-safety-relevant data other than label data that needs to be collected and/or retained for inclusion in documents such as certificates, reports, guarantees, and letters. -Measurement: Association of numbers with physical quantities. This includes measurements of 'Colour', 'Firmness', 'Mass', 'Pathogen', 'Size', 'Temperature', and 'Water Content'. -Colour: a subtype of 'Measurement' that is self-evident.-Firmness: a subtype of 'Measurement' that refers to the degree of resistance to deformation. -Mass: a subtype of 'Measurement' that refers to the amount of substance by weight or volume. -Pathogen: a subtype of 'Measurement' that refers to a microorganism that causes disease. -Size: a subtype of 'Measurement' that refers to dimension (e.g., length or thickness) or surface area. -Temperature: a subtype of 'Measurement' that is self-evident.-Water Content: a subtype of 'Measurement' that refers to humidity or moisture. -Time Constraint: A temporal restriction, in our context, is expressed using intervals, deadlines or periodicity. -Overall: requirements-related provisions that include all the introduced concepts.\"\"\"\n",
        "delimiter=\"%%\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eCSoRGOJ_7X"
      },
      "source": [
        "Code for getting responses from the  fine-tuned model on whole test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCI7jHZDKRg_"
      },
      "outputs": [],
      "source": [
        "def response_to_dataframe(response):\n",
        "    extracted_data = []\n",
        "    lines = response.strip().split('####')  # Process all lines\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:  # skip empty lines after splitting\n",
        "            continue\n",
        "\n",
        "        parts = line.split(':::', 1)\n",
        "\n",
        "        if len(parts) == 2:\n",
        "            sentence, labels = parts\n",
        "            sentence = sentence.strip(' \"')\n",
        "            labels = labels.strip()\n",
        "        elif len(parts) == 1:\n",
        "            print(f\"Only sentence found (no label): {line}\")  # Log the issue for visibility\n",
        "            sentence = parts[0].strip(' \"')\n",
        "            labels = None\n",
        "        else:\n",
        "            print(f\"Skipped line (improper format): {line}\")  # Log the issue for visibility\n",
        "            continue\n",
        "\n",
        "        extracted_data.append((sentence, labels))\n",
        "\n",
        "    df = pd.DataFrame(extracted_data, columns=[\"sentence\", \"label\"])\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASUBZDBGPTRD"
      },
      "outputs": [],
      "source": [
        "final_df = pd.DataFrame(columns=[\"sentence\", \"label\", \"paragraph_id\"])  # Initialize an empty DataFrame\n",
        "\n",
        "# Initialize response_df here\n",
        "response_df = pd.DataFrame(columns=[\"response\", \"paragraph_id\"])\n",
        "\n",
        "# Iterate over the dataframe to get model's responses\n",
        "for index, row in df_paragraphs.iterrows():\n",
        "    time.sleep(5)\n",
        "    paragraph = row['Statement']\n",
        "    paragraph_id = row['paragraph_id']\n",
        "    user_message=f\"%%{paragraph}%%\"\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': system_message},\n",
        "        {'role': 'user', 'content': user_message}\n",
        "    ]\n",
        "    response = get_completion_from_messages(messages)\n",
        "\n",
        "    # Append the response and its associated paragraph_id to response_df\n",
        "    response_df.loc[len(response_df)] = [response, paragraph_id]\n",
        "\n",
        "    # Convert the response to a dataframe\n",
        "    current_df = response_to_dataframe(response)\n",
        "    current_df[\"paragraph_id\"] = paragraph_id  # Add the paragraph id\n",
        "\n",
        "    # Directly append to final_df\n",
        "    final_df = pd.concat([final_df, current_df], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MZuSsawMxSR"
      },
      "outputs": [],
      "source": [
        "response_df.to_csv('response.csv', encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmwwpvlWMxYB"
      },
      "outputs": [],
      "source": [
        "final_df.to_csv('finaldf.csv', encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXrRFVZmL6Ev"
      },
      "outputs": [],
      "source": [
        "def get_best_match(sentence, indexed_sentences_list,paragraph_id,threshold,ngram_threshold, ngram_n=3):\n",
        "    # Strip single quotes from the beginning and end of the sentence\n",
        "    sentence=sentence.strip(\"'\\\"\")\n",
        "    # First, check for substring matches\n",
        "    for idx, s in indexed_sentences_list:\n",
        "        if sentence in s:\n",
        "            # print(\"substring match\", s)\n",
        "            return idx,s\n",
        "\n",
        "    lowered_sentence=lowercase_first_letter(sentence)\n",
        "\n",
        "    for idx, s in indexed_sentences_list:\n",
        "      if lowered_sentence in s:\n",
        "          return idx,s\n",
        "\n",
        "    for idx, s in indexed_sentences_list:\n",
        "      if s in sentence:\n",
        "          # print(\"Gt substring of S\", s)\n",
        "          return idx,s\n",
        "\n",
        "\n",
        "    # If no exact substring match is found, check for similarity using Levenshtein distance\n",
        "    best_match = None\n",
        "    min_distance = float('inf')  # Initialize to a large value\n",
        "\n",
        "    best_Levenshtein_match_idx=0\n",
        "    for idx, s in indexed_sentences_list:\n",
        "        distance = Levenshtein.distance(lowered_sentence, s)\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            best_match = s\n",
        "            best_Levenshtein_match_idx=idx\n",
        "\n",
        "    # Convert the distance to a similarity ratio\n",
        "    similarity_ratio = 1 - min_distance / max(len(lowered_sentence), len(best_match))\n",
        "\n",
        "    if similarity_ratio > threshold:\n",
        "        # print(\"best_Levenshtein_match_idx\", best_match)\n",
        "        return best_Levenshtein_match_idx,best_match\n",
        "\n",
        "    elif similarity_ratio < threshold:\n",
        "    # Check for substring matches\n",
        "      clean_sentence = ''.join(e for e in sentence if e.isalnum() or e.isspace()).strip()\n",
        "      for idx, s in indexed_sentences_list:\n",
        "          clean_s = ''.join(e for e in s if e.isalnum() or e.isspace()).strip()\n",
        "          if clean_sentence in clean_s:\n",
        "              return idx,s\n",
        "\n",
        "      best_ngram_similarity = 0\n",
        "      best_ngram_match = None\n",
        "      best_ngram_match_idx=0\n",
        "\n",
        "      sentence_ngrams = ngrams(sentence, ngram_n)\n",
        "      for idx, s in indexed_sentences_list:\n",
        "          s_ngrams = ngrams(s, ngram_n)\n",
        "          similarity = jaccard_similarity(sentence_ngrams, s_ngrams)\n",
        "\n",
        "          if similarity > best_ngram_similarity:\n",
        "              best_ngram_similarity = similarity\n",
        "              best_ngram_match = s\n",
        "              best_ngram_match_idx=idx\n",
        "\n",
        "      if best_ngram_similarity >= ngram_threshold:\n",
        "          return best_ngram_match_idx,best_ngram_match\n",
        "\n",
        "      # print(f\"Paragraph ID: {paragraph_id}\")\n",
        "      # print(f\"Unmatched sentence: {sentence}\")\n",
        "      # print(f\"Best Levenshtein match with similarity ratio: {similarity_ratio} was: {best_match}\")\n",
        "      # print(f\"Best n-gram match with similarity: {best_ngram_similarity} was: {best_ngram_match}\")\n",
        "      # print(\"-----\")\n",
        "\n",
        "      # with open('output.txt', 'a') as file:\n",
        "      #   file.write(f\"Paragraph ID: {paragraph_id}\\n\")\n",
        "      #   file.write(f\"Unmatched sentence: {sentence}\\n\")\n",
        "      #   file.write(f\"Best match with similarity ratio: {similarity_ratio} was: {best_match}\\n\")\n",
        "      #   file.write(\"-----\\n\")\n",
        "      return None,None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5wQ6HfTkECl"
      },
      "outputs": [],
      "source": [
        "# Compute precision, recall, and F-score for each label\n",
        "def compute_scores(tp, fp, fn):\n",
        "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "    fscore = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "    return precision, recall, fscore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jk-oMQJFlkgo"
      },
      "outputs": [],
      "source": [
        "labels_list = [\"Overall\",\"Data\", \"LabelData\", \"Non-labelData\", \"Measurement\", \"Colour\", \"Firmness\", \"Mass\", \"Pathogen\", \"Size\", \"Temperature\", \"Water Content\", \"Time Constraint\"]\n",
        "\n",
        "# Convert the labels list to lowercase for case-insensitivity (optional)\n",
        "labels_list = [label.lower() for label in labels_list]\n",
        "\n",
        "# Initialize all label columns with 0\n",
        "for label in labels_list:\n",
        "    final_df[label] = 0\n",
        "\n",
        "# Update the label columns based on the labels column\n",
        "for index, row in final_df.iterrows():\n",
        "\n",
        "    # Check if the label is a string\n",
        "    if isinstance(row['label'], str):\n",
        "        labels = [label.strip().lower() for label in row['label'].split(',')]  # Stripping whitespace and converting to lowercase\n",
        "        for label in labels:\n",
        "            if label in labels_list:\n",
        "                final_df.at[index, label] = 1\n",
        "            else:\n",
        "                print(f\"Unrecognized label: {label} in row {index}\")\n",
        "                print(final_df['sentence'].iloc[index])\n",
        "                print(\"+++++++\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R2RQ9eQlkkB"
      },
      "outputs": [],
      "source": [
        "measurement_subtypes = [\"Colour\", \"Firmness\", \"Mass\", \"Size\", \"Temperature\", \"Water Content\",\"Pathogen\"]\n",
        "data_subtypes = [\"LabelData\", \"Non-labelData\"]\n",
        "\n",
        "\n",
        "measurement_subtypes= [label.lower() for label in measurement_subtypes]\n",
        "data_subtypes= [label.lower() for label in data_subtypes]\n",
        "\n",
        "for index, row in final_df.iterrows():\n",
        "    # Set 'Overall' to 1 if any label is found\n",
        "    if any(row[label] == 1 for label in labels_list):\n",
        "        final_df.at[index, 'overall'] = 1\n",
        "\n",
        "    # Set 'Measurement' to 1 if any of its subtypes are found\n",
        "    if any(row[subtype] == 1 for subtype in measurement_subtypes):\n",
        "        final_df.at[index, 'measurement'] = 1\n",
        "\n",
        "    # Set 'Data' to 1 if any of its subtypes are found\n",
        "    if any(row[subtype] == 1 for subtype in data_subtypes):\n",
        "        final_df.at[index, 'data'] = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJhP6gfolkn3"
      },
      "outputs": [],
      "source": [
        "counters = {label: {\"TP\": 0, \"FP\": 0, \"FN\": 0} for label in labels_list}\n",
        "\n",
        "df_sentences.columns = [col.lower() for col in df_sentences.columns]\n",
        "unmatched_gt_sentences = {index: statement for index, statement in df_sentences['statement'].items()}\n",
        "matched_extracted_sentences = defaultdict(list)  # using collections.defaultdict\n",
        "# Dictionary to store missing labels for each matched ground truth sentence\n",
        "missing_labels_dict = {\n",
        "    (index, statement): [label for label in labels_list if df_sentences.loc[index, label] == 1]\n",
        "    for index, statement in df_sentences['statement'].items()\n",
        "}\n",
        "\n",
        "for label in labels_list:\n",
        "    init_count = sum([1 for labels in missing_labels_dict.values() if label in labels])\n",
        "    assert init_count == df_sentences[label].sum(), f\"Mismatch for {label} during initialization\"\n",
        "\n",
        "for index, row in final_df.iterrows():\n",
        "# for index, row in final_df.iloc[:5].iterrows():\n",
        "    extracted_sentence = row['sentence']\n",
        "    paragraph_id = row['paragraph_id']\n",
        "    # print(\"extracted_sentence: \", extracted_sentence)\n",
        "\n",
        "    # Filter the ground truth dataframe by the current paragraph_id\n",
        "    filtered_gt_df = df_sentences[df_sentences['paragraph_id'] == paragraph_id]\n",
        "\n",
        "    # Prepare a list of tuples (index, statement)\n",
        "    indexed_statements = list(filtered_gt_df[['statement']].itertuples(index=True, name=None))\n",
        "\n",
        "    # Find the best matching ground truth sentence and its index\n",
        "    matched_index, matched_sentence = get_best_match(extracted_sentence, indexed_statements, paragraph_id,threshold=0.90,ngram_threshold=0.90)\n",
        "\n",
        "    # If there's a match, compare each label\n",
        "    #if matched_index and matched_sentence:\n",
        "    if matched_index is not None and matched_sentence is not None:\n",
        "\n",
        "        matched_extracted_sentences[(matched_index, matched_sentence)].append(extracted_sentence)\n",
        "        # Remove the matched sentence from unmatched_gt_sentences\n",
        "        unmatched_gt_sentences.pop(matched_index, None)\n",
        "        # print(\"matched_sentence: \", matched_sentence)\n",
        "\n",
        "        ground_truth_row = filtered_gt_df.loc[filtered_gt_df['statement'] == matched_sentence].iloc[0]\n",
        "\n",
        "        for label in labels_list:\n",
        "            # Extracted is 1, Ground Truth is 1: True Positive\n",
        "            if row[label] == 1 and ground_truth_row[label] == 1:\n",
        "                if label in missing_labels_dict[matched_index,matched_sentence]:  # Only if the label is still missing\n",
        "                  counters[label][\"TP\"] += 1\n",
        "                  missing_labels_dict[(matched_index, matched_sentence)].remove(label)  # Remove the label as it's no longer missing\n",
        "\n",
        "            # Extracted is 1, Ground Truth is 0: False Positive\n",
        "            elif row[label] == 1 and ground_truth_row[label] == 0:\n",
        "                counters[label][\"FP\"] += 1\n",
        "    else:\n",
        "        # print(\"unmatched extracted sentence: \", extracted_sentence)\n",
        "\n",
        "        for label in labels_list:\n",
        "            if row[label] == 1:\n",
        "                counters[label][\"FP\"] += 1\n",
        "\n",
        "# FN count is incremented for missing labels in the gt sentences.\n",
        "for sentence, missing_labels in missing_labels_dict.items():\n",
        "    for label in missing_labels:\n",
        "        counters[label][\"FN\"] += 1\n",
        "\n",
        "\n",
        "scores = {}\n",
        "for label, counts in counters.items():\n",
        "    precision, recall, fscore = compute_scores(counts[\"TP\"], counts[\"FP\"], counts[\"FN\"])\n",
        "    scores[label] = {\"Precision\": precision, \"Recall\": recall, \"F-score\": fscore}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeRUelZ9luYB"
      },
      "outputs": [],
      "source": [
        "with open('output.txt', 'a') as f:\n",
        "    f.write(str(counters))\n",
        "    f.write('\\n\\n')  # Separate the two dictionaries with two newlines\n",
        "    f.write(str(scores))\n",
        "    f.write('\\n\\n')  # Separate the two dictionaries with two newlines"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
